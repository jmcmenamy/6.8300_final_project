{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusion training in Fourier domain"
      ],
      "metadata": {
        "id": "7jFk8RJdU1Rn"
      },
      "id": "7jFk8RJdU1Rn"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "\n",
        "class GradualWarmupScheduler(_LRScheduler):\n",
        "    def __init__(self, optimizer, multiplier, warm_epoch, after_scheduler=None):\n",
        "        self.multiplier = multiplier\n",
        "        self.total_epoch = warm_epoch\n",
        "        self.after_scheduler = after_scheduler\n",
        "        self.finished = False\n",
        "        self.last_epoch = None\n",
        "        self.base_lrs = None\n",
        "        super().__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self.last_epoch > self.total_epoch:\n",
        "            if self.after_scheduler:\n",
        "                if not self.finished:\n",
        "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
        "                    self.finished = True\n",
        "                return self.after_scheduler.get_lr()\n",
        "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
        "        return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
        "\n",
        "\n",
        "    def step(self, epoch=None, metrics=None):\n",
        "        if self.finished and self.after_scheduler:\n",
        "            if epoch is None:\n",
        "                self.after_scheduler.step(None)\n",
        "            else:\n",
        "                self.after_scheduler.step(epoch - self.total_epoch)\n",
        "        else:\n",
        "            return super(GradualWarmupScheduler, self).step(epoch)"
      ],
      "metadata": {
        "id": "plVDGW5PXZ9t",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1747098364939,
          "user_tz": 240,
          "elapsed": 3655,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "plVDGW5PXZ9t",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ModelCondition.py\n",
        "\n",
        "import math\n",
        "from telnetlib import PRAGMA_HEARTBEAT\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import init\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "def drop_connect(x, drop_ratio):\n",
        "    keep_ratio = 1.0 - drop_ratio\n",
        "    mask = torch.empty([x.shape[0], 1, 1, 1], dtype=x.dtype, device=x.device)\n",
        "    mask.bernoulli_(p=keep_ratio)\n",
        "    x.div_(keep_ratio)\n",
        "    x.mul_(mask)\n",
        "    return x\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class TimeEmbedding(nn.Module):\n",
        "    def __init__(self, T, d_model, dim):\n",
        "        assert d_model % 2 == 0\n",
        "        super().__init__()\n",
        "        emb = torch.arange(0, d_model, step=2) / d_model * math.log(10000)\n",
        "        emb = torch.exp(-emb)\n",
        "        pos = torch.arange(T).float()\n",
        "        emb = pos[:, None] * emb[None, :]\n",
        "        assert list(emb.shape) == [T, d_model // 2]\n",
        "        emb = torch.stack([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
        "        assert list(emb.shape) == [T, d_model // 2, 2]\n",
        "        emb = emb.view(T, d_model)\n",
        "\n",
        "        self.timembedding = nn.Sequential(\n",
        "            nn.Embedding.from_pretrained(emb, freeze=False),\n",
        "            nn.Linear(d_model, dim),\n",
        "            Swish(),\n",
        "            nn.Linear(dim, dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, t):\n",
        "        emb = self.timembedding(t)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class ConditionalEmbedding(nn.Module):\n",
        "    def __init__(self, num_labels, d_model, dim):\n",
        "        assert d_model % 2 == 0\n",
        "        super().__init__()\n",
        "        self.condEmbedding = nn.Sequential(\n",
        "            nn.Embedding(num_embeddings=num_labels + 1, embedding_dim=d_model, padding_idx=0),\n",
        "            nn.Linear(d_model, dim),\n",
        "            Swish(),\n",
        "            nn.Linear(dim, dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, t):\n",
        "        emb = self.condEmbedding(t)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class DownSample(nn.Module):\n",
        "    def __init__(self, in_ch):\n",
        "        super().__init__()\n",
        "        self.c1 = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1)\n",
        "        self.c2 = nn.Conv2d(in_ch, in_ch, 5, stride=2, padding=2)\n",
        "\n",
        "    def forward(self, x, temb, cemb):\n",
        "        x = self.c1(x) + self.c2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpSample(nn.Module):\n",
        "    def __init__(self, in_ch):\n",
        "        super().__init__()\n",
        "        self.c = nn.Conv2d(in_ch, in_ch, 3, stride=1, padding=1)\n",
        "        self.t = nn.ConvTranspose2d(in_ch, in_ch, 5, 2, 2, 1)\n",
        "\n",
        "    def forward(self, x, temb, cemb):\n",
        "        _, _, H, W = x.shape\n",
        "        x = self.t(x)\n",
        "        x = self.c(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    def __init__(self, in_ch):\n",
        "        super().__init__()\n",
        "        self.group_norm = nn.GroupNorm(24, in_ch)\n",
        "        self.proj_q = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
        "        self.proj_k = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
        "        self.proj_v = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
        "        self.proj = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        h = self.group_norm(x)\n",
        "        q = self.proj_q(h)\n",
        "        k = self.proj_k(h)\n",
        "        v = self.proj_v(h)\n",
        "\n",
        "        q = q.permute(0, 2, 3, 1).view(B, H * W, C)\n",
        "        k = k.view(B, C, H * W)\n",
        "        w = torch.bmm(q, k) * (int(C) ** (-0.5))\n",
        "        assert list(w.shape) == [B, H * W, H * W]\n",
        "        w = F.softmax(w, dim=-1)\n",
        "\n",
        "        v = v.permute(0, 2, 3, 1).view(B, H * W, C)\n",
        "        h = torch.bmm(w, v)\n",
        "        assert list(h.shape) == [B, H * W, C]\n",
        "        h = h.view(B, H, W, C).permute(0, 3, 1, 2)\n",
        "        h = self.proj(h)\n",
        "\n",
        "        return x + h\n",
        "\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, tdim, dropout, attn=True):\n",
        "        super().__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.GroupNorm(24, in_ch),\n",
        "            Swish(),\n",
        "            nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1),\n",
        "        )\n",
        "        self.temb_proj = nn.Sequential(\n",
        "            Swish(),\n",
        "            nn.Linear(tdim, out_ch),\n",
        "        )\n",
        "        self.cond_proj = nn.Sequential(\n",
        "            Swish(),\n",
        "            nn.Linear(tdim, out_ch),\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.GroupNorm(24, out_ch),\n",
        "            Swish(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1),\n",
        "        )\n",
        "        if in_ch != out_ch:\n",
        "            self.shortcut = nn.Conv2d(in_ch, out_ch, 1, stride=1, padding=0)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "        if attn:\n",
        "            self.attn = AttnBlock(out_ch)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "\n",
        "\n",
        "    def forward(self, x, temb, labels):\n",
        "        h = self.block1(x)\n",
        "        h += self.temb_proj(temb)[:, :, None, None]\n",
        "        h += self.cond_proj(labels)[:, :, None, None]\n",
        "        h = self.block2(h)\n",
        "\n",
        "        h = h + self.shortcut(x)\n",
        "        h = self.attn(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, T, num_labels, ch, ch_mult, num_res_blocks, dropout):\n",
        "        super().__init__()\n",
        "        tdim = ch * 4\n",
        "        self.time_embedding = TimeEmbedding(T, ch, tdim)\n",
        "        self.cond_embedding = ConditionalEmbedding(num_labels, ch, tdim)\n",
        "        self.head = nn.Conv2d(3, ch, kernel_size=3, stride=1, padding=1)\n",
        "        self.downblocks = nn.ModuleList()\n",
        "        chs = [ch]  # record output channel when dowmsample for upsample\n",
        "        now_ch = ch\n",
        "        for i, mult in enumerate(ch_mult):\n",
        "            out_ch = ch * mult\n",
        "            for _ in range(num_res_blocks):\n",
        "                self.downblocks.append(ResBlock(in_ch=now_ch, out_ch=out_ch, tdim=tdim, dropout=dropout))\n",
        "                now_ch = out_ch\n",
        "                chs.append(now_ch)\n",
        "            if i != len(ch_mult) - 1:\n",
        "                self.downblocks.append(DownSample(now_ch))\n",
        "                chs.append(now_ch)\n",
        "\n",
        "        self.middleblocks = nn.ModuleList([\n",
        "            ResBlock(now_ch, now_ch, tdim, dropout, attn=True),\n",
        "            ResBlock(now_ch, now_ch, tdim, dropout, attn=False),\n",
        "        ])\n",
        "\n",
        "        self.upblocks = nn.ModuleList()\n",
        "        for i, mult in reversed(list(enumerate(ch_mult))):\n",
        "            out_ch = ch * mult\n",
        "            for _ in range(num_res_blocks + 1):\n",
        "                self.upblocks.append(ResBlock(in_ch=chs.pop() + now_ch, out_ch=out_ch, tdim=tdim, dropout=dropout, attn=False))\n",
        "                now_ch = out_ch\n",
        "            if i != 0:\n",
        "                self.upblocks.append(UpSample(now_ch))\n",
        "        assert len(chs) == 0\n",
        "\n",
        "        self.tail = nn.Sequential(\n",
        "            nn.GroupNorm(32, now_ch),\n",
        "            Swish(),\n",
        "            nn.Conv2d(now_ch, 3, 3, stride=1, padding=1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, t, labels):\n",
        "        # Timestep embedding\n",
        "        temb = self.time_embedding(t)\n",
        "        cemb = self.cond_embedding(labels)\n",
        "        # Downsampling\n",
        "        h = self.head(x)\n",
        "        hs = [h]\n",
        "        for layer in self.downblocks:\n",
        "            h = layer(h, temb, cemb)\n",
        "            hs.append(h)\n",
        "        # Middle\n",
        "        for layer in self.middleblocks:\n",
        "            h = layer(h, temb, cemb)\n",
        "        # Upsampling\n",
        "        for layer in self.upblocks:\n",
        "            if isinstance(layer, ResBlock):\n",
        "                h = torch.cat([h, hs.pop()], dim=1)\n",
        "            h = layer(h, temb, cemb)\n",
        "        h = self.tail(h)\n",
        "\n",
        "        assert len(hs) == 0\n",
        "        return h\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     batch_size = 8\n",
        "#     model = UNet(\n",
        "#         T=1000, num_labels=10, ch=128, ch_mult=[1, 2, 2, 2],\n",
        "#         num_res_blocks=2, dropout=0.1)\n",
        "#     x = torch.randn(batch_size, 3, 24, 24)\n",
        "#     t = torch.randint(1000, size=[batch_size])\n",
        "#     labels = torch.randint(10, size=[batch_size])\n",
        "#     # resB = ResBlock(128, 256, 64, 0.1)\n",
        "#     # x = torch.randn(batch_size, 128, 32, 32)\n",
        "#     # t = torch.randn(batch_size, 64)\n",
        "#     # labels = torch.randn(batch_size, 64)\n",
        "#     # y = resB(x, t, labels)\n",
        "#     y = model(x, t, labels)\n",
        "#     print(y.shape)"
      ],
      "metadata": {
        "id": "wQl_l7v3XLc_",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1747101827871,
          "user_tz": 240,
          "elapsed": 3606,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "wQl_l7v3XLc_",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DiffusionCondition.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def extract(v, t, x_shape):\n",
        "    \"\"\"\n",
        "    Extract some coefficients at specified timesteps, then reshape to\n",
        "    [batch_size, 1, 1, 1, 1, ...] for broadcasting purposes.\n",
        "    \"\"\"\n",
        "    device = t.device\n",
        "    out = torch.gather(v, index=t, dim=0).float().to(device)\n",
        "    return out.view([t.shape[0]] + [1] * (len(x_shape) - 1))\n",
        "\n",
        "\n",
        "class GaussianDiffusionTrainer(nn.Module):\n",
        "    def __init__(self, model, beta_1, beta_T, T):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.T = T\n",
        "\n",
        "        self.register_buffer(\n",
        "            'betas', torch.linspace(beta_1, beta_T, T).double())\n",
        "        alphas = 1. - self.betas\n",
        "        alphas_bar = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
        "        self.register_buffer(\n",
        "            'sqrt_alphas_bar', torch.sqrt(alphas_bar))\n",
        "        self.register_buffer(\n",
        "            'sqrt_one_minus_alphas_bar', torch.sqrt(1. - alphas_bar))\n",
        "\n",
        "    def forward(self, x_0, labels):\n",
        "        \"\"\"\n",
        "        Algorithm 1.\n",
        "        \"\"\"\n",
        "        t = torch.randint(self.T, size=(x_0.shape[0], ), device=x_0.device)\n",
        "        noise = torch.randn_like(x_0)\n",
        "        x_t =   extract(self.sqrt_alphas_bar, t, x_0.shape) * x_0 + \\\n",
        "                extract(self.sqrt_one_minus_alphas_bar, t, x_0.shape) * noise\n",
        "        loss = F.mse_loss(self.model(x_t, t, labels), noise, reduction='none')\n",
        "        return loss\n",
        "\n",
        "\n",
        "class GaussianDiffusionSampler(nn.Module):\n",
        "    def __init__(self, model, beta_1, beta_T, T, w = 0.):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.T = T\n",
        "        ### In the classifier free guidence paper, w is the key to control the gudience.\n",
        "        ### w = 0 and with label = 0 means no guidence.\n",
        "        ### w > 0 and label > 0 means guidence. Guidence would be stronger if w is bigger.\n",
        "        self.w = w\n",
        "\n",
        "        self.register_buffer('betas', torch.linspace(beta_1, beta_T, T).double())\n",
        "        alphas = 1. - self.betas\n",
        "        alphas_bar = torch.cumprod(alphas, dim=0)\n",
        "        alphas_bar_prev = F.pad(alphas_bar, [1, 0], value=1)[:T]\n",
        "        self.register_buffer('coeff1', torch.sqrt(1. / alphas))\n",
        "        self.register_buffer('coeff2', self.coeff1 * (1. - alphas) / torch.sqrt(1. - alphas_bar))\n",
        "        self.register_buffer('posterior_var', self.betas * (1. - alphas_bar_prev) / (1. - alphas_bar))\n",
        "\n",
        "    def predict_xt_prev_mean_from_eps(self, x_t, t, eps):\n",
        "        assert x_t.shape == eps.shape\n",
        "        return extract(self.coeff1, t, x_t.shape) * x_t - extract(self.coeff2, t, x_t.shape) * eps\n",
        "\n",
        "    def p_mean_variance(self, x_t, t, labels):\n",
        "        # below: only log_variance is used in the KL computations\n",
        "        var = torch.cat([self.posterior_var[1:2], self.betas[1:]])\n",
        "        var = extract(var, t, x_t.shape)\n",
        "        eps = self.model(x_t, t, labels)\n",
        "        nonEps = self.model(x_t, t, torch.zeros_like(labels).to(labels.device))\n",
        "        eps = (1. + self.w) * eps - self.w * nonEps\n",
        "        xt_prev_mean = self.predict_xt_prev_mean_from_eps(x_t, t, eps=eps)\n",
        "        return xt_prev_mean, var\n",
        "\n",
        "    def forward(self, x_T, labels):\n",
        "        \"\"\"\n",
        "        Algorithm 2.\n",
        "        \"\"\"\n",
        "        x_t = x_T\n",
        "        for time_step in reversed(range(self.T)):\n",
        "            print(time_step)\n",
        "            t = x_t.new_ones([x_T.shape[0], ], dtype=torch.long) * time_step\n",
        "            mean, var= self.p_mean_variance(x_t=x_t, t=t, labels=labels)\n",
        "            if time_step > 0:\n",
        "                noise = torch.randn_like(x_t)\n",
        "            else:\n",
        "                noise = 0\n",
        "            x_t = mean + torch.sqrt(var) * noise\n",
        "            assert torch.isnan(x_t).int().sum() == 0, \"nan in tensor.\"\n",
        "        x_0 = x_t\n",
        "        return torch.clip(x_0, -1, 1)"
      ],
      "metadata": {
        "id": "FCFzk2lyXAqW",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1747101842489,
          "user_tz": 240,
          "elapsed": 615,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "FCFzk2lyXAqW",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TrainCondition.py\n",
        "\n",
        "import os\n",
        "from typing import Dict\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "## new\n",
        "import torchvision.transforms.functional as TF  # DCT\n",
        "from scipy.fftpack import dct, idct\n",
        "import numpy as np\n",
        "\n",
        "def dct_batch(images: torch.Tensor) -> torch.Tensor:\n",
        "    # Convert to numpy and apply DCT per channel\n",
        "    images_np = images.cpu().numpy()\n",
        "    B, C, H, W = images_np.shape\n",
        "    dct_images = np.zeros_like(images_np)\n",
        "\n",
        "    for b in range(B):\n",
        "        for c in range(C):\n",
        "            dct_images[b, c] = dct(dct(images_np[b, c], axis=0, norm='ortho'), axis=1, norm='ortho')\n",
        "    compressed = dct_images[:, :, :24, :24]\n",
        "    return torch.from_numpy(compressed).to(images.device)\n",
        "\n",
        "\n",
        "def idct_batch(dct_images: torch.Tensor) -> torch.Tensor:\n",
        "    # Convert to numpy and apply IDCT per channel\n",
        "    dct_np = dct_images.cpu().numpy()\n",
        "    B, C, H, W = dct_np.shape\n",
        "    recon_images = np.zeros_like(dct_np)\n",
        "\n",
        "    for b in range(B):\n",
        "        for c in range(C):\n",
        "            recon_images[b, c] = idct(idct(dct_np[b, c], axis=1, norm='ortho'), axis=0, norm='ortho')\n",
        "\n",
        "    return torch.from_numpy(recon_images).to(dct_images.device)\n",
        "##\n",
        "\n",
        "\n",
        "def train(modelConfig: Dict):\n",
        "    device = torch.device(modelConfig[\"device\"])\n",
        "    # dataset\n",
        "    dataset = CIFAR10(\n",
        "        root='./CIFAR10', train=True, download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ]))\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=modelConfig[\"batch_size\"], shuffle=True, num_workers=4, drop_last=True, pin_memory=True)\n",
        "\n",
        "    # model setup\n",
        "    net_model = UNet(T=modelConfig[\"T\"], num_labels=10, ch=modelConfig[\"channel\"], ch_mult=modelConfig[\"channel_mult\"],\n",
        "                     num_res_blocks=modelConfig[\"num_res_blocks\"], dropout=modelConfig[\"dropout\"]).to(device)\n",
        "    if modelConfig[\"training_load_weight\"] is not None:\n",
        "        net_model.load_state_dict(torch.load(os.path.join(\n",
        "            modelConfig[\"save_dir\"], modelConfig[\"training_load_weight\"]), map_location=device), strict=False)\n",
        "        print(\"Model weight load down.\")\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        net_model.parameters(), lr=modelConfig[\"lr\"], weight_decay=1e-4)\n",
        "    cosineScheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer=optimizer, T_max=modelConfig[\"epoch\"], eta_min=0, last_epoch=-1)\n",
        "    warmUpScheduler = GradualWarmupScheduler(optimizer=optimizer, multiplier=modelConfig[\"multiplier\"],\n",
        "                                             warm_epoch=modelConfig[\"epoch\"] // 10, after_scheduler=cosineScheduler)\n",
        "    trainer = GaussianDiffusionTrainer(\n",
        "        net_model, modelConfig[\"beta_1\"], modelConfig[\"beta_T\"], modelConfig[\"T\"]).to(device)\n",
        "\n",
        "    losses = []  # new\n",
        "\n",
        "    # start training\n",
        "    losses = []\n",
        "    lrs = []\n",
        "    for e in range(modelConfig[\"epoch\"]):\n",
        "        epoch_losses = []\n",
        "        epoch_lrs = []\n",
        "        with tqdm(dataloader, dynamic_ncols=True) as tqdmDataLoader:\n",
        "            for images_, labels in tqdmDataLoader:  # new var name\n",
        "                # train\n",
        "                images = dct_batch(images_)  # new\n",
        "                b = images.shape[0]\n",
        "                optimizer.zero_grad()\n",
        "                x_0 = images.to(device)\n",
        "                labels = labels.to(device) + 1\n",
        "                if np.random.rand() < 0.1:\n",
        "                    labels = torch.zeros_like(labels).to(device)\n",
        "                loss = trainer(x_0, labels).sum() / b ** 2.\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    net_model.parameters(), modelConfig[\"grad_clip\"])\n",
        "                optimizer.step()\n",
        "                loss_item = loss.item()\n",
        "                lr = optimizer.state_dict()['param_groups'][0][\"lr\"]\n",
        "                epoch_lrs.append(lr)\n",
        "                epoch_losses.append(loss_item)\n",
        "                tqdmDataLoader.set_postfix(ordered_dict={\n",
        "                    \"epoch\": e,\n",
        "                    \"loss: \": loss_item,\n",
        "                    \"img shape: \": x_0.shape,\n",
        "                    \"LR\": lr\n",
        "                })\n",
        "            tqdmDataLoader.set_postfix(ordered_dict={\n",
        "                \"epoch\": e,\n",
        "                \"loss: \": np.mean(epoch_losses),\n",
        "                \"img shape: \": x_0.shape,\n",
        "                \"LR\": np.mean(epoch_lrs)\n",
        "            })\n",
        "        warmUpScheduler.step()\n",
        "        losses.append(np.mean(epoch_losses))\n",
        "        lrs.append(np.mean(epoch_lrs))\n",
        "        torch.save(net_model.state_dict(), os.path.join(\n",
        "            modelConfig[\"save_dir\"], 'ckpt_' + str(e) + \"_.pt\"))\n",
        "    print(losses)\n",
        "    print(lrs)\n",
        "    with open('./data.txt', 'w') as f:\n",
        "        f.write(f\"{losses}\\n{lrs}\")\n",
        "\n",
        "    return losses  # new\n",
        "\n",
        "\n",
        "def eval(modelConfig: Dict):\n",
        "    device = torch.device(modelConfig[\"device\"])\n",
        "    # load model and evaluate\n",
        "    with torch.no_grad():\n",
        "        step = int(modelConfig[\"batch_size\"] // 10)\n",
        "        labelList = []\n",
        "        k = 0\n",
        "        for i in range(1, modelConfig[\"batch_size\"] + 1):\n",
        "            labelList.append(torch.ones(size=[1]).long() * k)\n",
        "            if i % step == 0:\n",
        "                if k < 10 - 1:\n",
        "                    k += 1\n",
        "        labels = torch.cat(labelList, dim=0).long().to(device) + 1\n",
        "        print(\"labels: \", labels)\n",
        "        model = UNet(T=modelConfig[\"T\"], num_labels=10, ch=modelConfig[\"channel\"], ch_mult=modelConfig[\"channel_mult\"],\n",
        "                     num_res_blocks=modelConfig[\"num_res_blocks\"], dropout=modelConfig[\"dropout\"]).to(device)\n",
        "        print(\"path is\", os.path.join(\n",
        "            modelConfig[\"save_dir\"], modelConfig[\"test_load_weight\"]))\n",
        "        ckpt = torch.load(os.path.join(\n",
        "            modelConfig[\"save_dir\"], modelConfig[\"test_load_weight\"]), map_location=device)\n",
        "        model.load_state_dict(ckpt)\n",
        "        print(\"model load weight done.\")\n",
        "        model.eval()\n",
        "        sampler = GaussianDiffusionSampler(\n",
        "            model, modelConfig[\"beta_1\"], modelConfig[\"beta_T\"], modelConfig[\"T\"], w=modelConfig[\"w\"]).to(device)\n",
        "        # Sampled from standard normal distribution\n",
        "        noisyImage = torch.randn(\n",
        "            size=[modelConfig[\"batch_size\"], 3, modelConfig[\"img_size\"], modelConfig[\"img_size\"]], device=device)\n",
        "        saveNoisy = torch.clamp(noisyImage * 0.5 + 0.5, 0, 1)\n",
        "        save_image(saveNoisy, os.path.join(\n",
        "            modelConfig[\"sampled_dir\"],  modelConfig[\"sampledNoisyImgName\"]), nrow=modelConfig[\"nrow\"])\n",
        "        sampledImgs = sampler(noisyImage, labels)\n",
        "        sampledImgs = sampledImgs * 0.5 + 0.5  # [0 ~ 1]\n",
        "        print(sampledImgs)\n",
        "        save_image(sampledImgs, os.path.join(\n",
        "            modelConfig[\"sampled_dir\"],  modelConfig[\"sampledImgName\"]), nrow=modelConfig[\"nrow\"])"
      ],
      "metadata": {
        "id": "Gbd4p4yVWxpX",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1747102263965,
          "user_tz": 240,
          "elapsed": 865,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "Gbd4p4yVWxpX",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "wCYeaIWBABHU6QenKhzWwsX3",
      "metadata": {
        "tags": [],
        "id": "wCYeaIWBABHU6QenKhzWwsX3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0c2c1151-21a3-45fd-be2c-0d154c518e04",
        "collapsed": true,
        "executionInfo": {
          "status": "error",
          "timestamp": 1747099194116,
          "user_tz": 240,
          "elapsed": 98532,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "source": [
        "# main.py\n",
        "\n",
        "def main(model_config=None):\n",
        "    modelConfig = {\n",
        "        \"state\": \"eval\", # or eval\n",
        "        \"epoch\": 70,\n",
        "        \"batch_size\": 80,\n",
        "        \"T\": 500,\n",
        "        \"channel\": 128,\n",
        "        \"channel_mult\": [1, 2, 2, 2],\n",
        "        \"num_res_blocks\": 2,\n",
        "        \"dropout\": 0.15,\n",
        "        \"lr\": 1e-4,\n",
        "        \"multiplier\": 2.5,\n",
        "        \"beta_1\": 1e-4,\n",
        "        \"beta_T\": 0.028,\n",
        "        \"img_size\": 32,\n",
        "        \"grad_clip\": 1.,\n",
        "        \"device\": \"cuda:0\",\n",
        "        \"w\": 1.8,\n",
        "        \"save_dir\": \"./CheckpointsCondition\",\n",
        "        \"training_load_weight\": None,\n",
        "        \"test_load_weight\": \"../ckpt_69_fourier_new.pt\",\n",
        "        \"sampled_dir\": \"./SampledImgs/\",\n",
        "        \"sampledNoisyImgName\": \"NoisyGuidenceImgs.png\",\n",
        "        \"sampledImgName\": \"SampledGuidenceImgs.png\",\n",
        "        \"nrow\": 8\n",
        "    }\n",
        "    if model_config is not None:\n",
        "        modelConfig = model_config\n",
        "    if modelConfig[\"state\"] == \"train\":\n",
        "        training_loss = train(modelConfig)  # new\n",
        "        print(training_loss)  # new\n",
        "    else:\n",
        "        eval(modelConfig)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels:  tensor([ 1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,\n",
            "         3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,\n",
            "         5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,\n",
            "         7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,\n",
            "        10, 10, 10, 10, 10, 10, 10, 10], device='cuda:0')\n",
            "path is ./CheckpointsCondition/../ckpt_69_fourier_new.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-6bcaf096ff2d>:145: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(os.path.join(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model load weight done.\n",
            "499\n",
            "498\n",
            "497\n",
            "496\n",
            "495\n",
            "494\n",
            "493\n",
            "492\n",
            "491\n",
            "490\n",
            "489\n",
            "488\n",
            "487\n",
            "486\n",
            "485\n",
            "484\n",
            "483\n",
            "482\n",
            "481\n",
            "480\n",
            "479\n",
            "478\n",
            "477\n",
            "476\n",
            "475\n",
            "474\n",
            "473\n",
            "472\n",
            "471\n",
            "470\n",
            "469\n",
            "468\n",
            "467\n",
            "466\n",
            "465\n",
            "464\n",
            "463\n",
            "462\n",
            "461\n",
            "460\n",
            "459\n",
            "458\n",
            "457\n",
            "456\n",
            "455\n",
            "454\n",
            "453\n",
            "452\n",
            "451\n",
            "450\n",
            "449\n",
            "448\n",
            "447\n",
            "446\n",
            "445\n",
            "444\n",
            "443\n",
            "442\n",
            "441\n",
            "440\n",
            "439\n",
            "438\n",
            "437\n",
            "436\n",
            "435\n",
            "434\n",
            "433\n",
            "432\n",
            "431\n",
            "430\n",
            "429\n",
            "428\n",
            "427\n",
            "426\n",
            "425\n",
            "424\n",
            "423\n",
            "422\n",
            "421\n",
            "420\n",
            "419\n",
            "418\n",
            "417\n",
            "416\n",
            "415\n",
            "414\n",
            "413\n",
            "412\n",
            "411\n",
            "410\n",
            "409\n",
            "408\n",
            "407\n",
            "406\n",
            "405\n",
            "404\n",
            "403\n",
            "402\n",
            "401\n",
            "400\n",
            "399\n",
            "398\n",
            "397\n",
            "396\n",
            "395\n",
            "394\n",
            "393\n",
            "392\n",
            "391\n",
            "390\n",
            "389\n",
            "388\n",
            "387\n",
            "386\n",
            "385\n",
            "384\n",
            "383\n",
            "382\n",
            "381\n",
            "380\n",
            "379\n",
            "378\n",
            "377\n",
            "376\n",
            "375\n",
            "374\n",
            "373\n",
            "372\n",
            "371\n",
            "370\n",
            "369\n",
            "368\n",
            "367\n",
            "366\n",
            "365\n",
            "364\n",
            "363\n",
            "362\n",
            "361\n",
            "360\n",
            "359\n",
            "358\n",
            "357\n",
            "356\n",
            "355\n",
            "354\n",
            "353\n",
            "352\n",
            "351\n",
            "350\n",
            "349\n",
            "348\n",
            "347\n",
            "346\n",
            "345\n",
            "344\n",
            "343\n",
            "342\n",
            "341\n",
            "340\n",
            "339\n",
            "338\n",
            "337\n",
            "336\n",
            "335\n",
            "334\n",
            "333\n",
            "332\n",
            "331\n",
            "330\n",
            "329\n",
            "328\n",
            "327\n",
            "326\n",
            "325\n",
            "324\n",
            "323\n",
            "322\n",
            "321\n",
            "320\n",
            "319\n",
            "318\n",
            "317\n",
            "316\n",
            "315\n",
            "314\n",
            "313\n",
            "312\n",
            "311\n",
            "310\n",
            "309\n",
            "308\n",
            "307\n",
            "306\n",
            "305\n",
            "304\n",
            "303\n",
            "302\n",
            "301\n",
            "300\n",
            "299\n",
            "298\n",
            "297\n",
            "296\n",
            "295\n",
            "294\n",
            "293\n",
            "292\n",
            "291\n",
            "290\n",
            "289\n",
            "288\n",
            "287\n",
            "286\n",
            "285\n",
            "284\n",
            "283\n",
            "282\n",
            "281\n",
            "280\n",
            "279\n",
            "278\n",
            "277\n",
            "276\n",
            "275\n",
            "274\n",
            "273\n",
            "272\n",
            "271\n",
            "270\n",
            "269\n",
            "268\n",
            "267\n",
            "266\n",
            "265\n",
            "264\n",
            "263\n",
            "262\n",
            "261\n",
            "260\n",
            "259\n",
            "258\n",
            "257\n",
            "256\n",
            "255\n",
            "254\n",
            "253\n",
            "252\n",
            "251\n",
            "250\n",
            "249\n",
            "248\n",
            "247\n",
            "246\n",
            "245\n",
            "244\n",
            "243\n",
            "242\n",
            "241\n",
            "240\n",
            "239\n",
            "238\n",
            "237\n",
            "236\n",
            "235\n",
            "234\n",
            "233\n",
            "232\n",
            "231\n",
            "230\n",
            "229\n",
            "228\n",
            "227\n",
            "226\n",
            "225\n",
            "224\n",
            "223\n",
            "222\n",
            "221\n",
            "220\n",
            "219\n",
            "218\n",
            "217\n",
            "216\n",
            "215\n",
            "214\n",
            "213\n",
            "212\n",
            "211\n",
            "210\n",
            "209\n",
            "208\n",
            "207\n",
            "206\n",
            "205\n",
            "204\n",
            "203\n",
            "202\n",
            "201\n",
            "200\n",
            "199\n",
            "198\n",
            "197\n",
            "196\n",
            "195\n",
            "194\n",
            "193\n",
            "192\n",
            "191\n",
            "190\n",
            "189\n",
            "188\n",
            "187\n",
            "186\n",
            "185\n",
            "184\n",
            "183\n",
            "182\n",
            "181\n",
            "180\n",
            "179\n",
            "178\n",
            "177\n",
            "176\n",
            "175\n",
            "174\n",
            "173\n",
            "172\n",
            "171\n",
            "170\n",
            "169\n",
            "168\n",
            "167\n",
            "166\n",
            "165\n",
            "164\n",
            "163\n",
            "162\n",
            "161\n",
            "160\n",
            "159\n",
            "158\n",
            "157\n",
            "156\n",
            "155\n",
            "154\n",
            "153\n",
            "152\n",
            "151\n",
            "150\n",
            "149\n",
            "148\n",
            "147\n",
            "146\n",
            "145\n",
            "144\n",
            "143\n",
            "142\n",
            "141\n",
            "140\n",
            "139\n",
            "138\n",
            "137\n",
            "136\n",
            "135\n",
            "134\n",
            "133\n",
            "132\n",
            "131\n",
            "130\n",
            "129\n",
            "128\n",
            "127\n",
            "126\n",
            "125\n",
            "124\n",
            "123\n",
            "122\n",
            "121\n",
            "120\n",
            "119\n",
            "118\n",
            "117\n",
            "116\n",
            "115\n",
            "114\n",
            "113\n",
            "112\n",
            "111\n",
            "110\n",
            "109\n",
            "108\n",
            "107\n",
            "106\n",
            "105\n",
            "104\n",
            "103\n",
            "102\n",
            "101\n",
            "100\n",
            "99\n",
            "98\n",
            "97\n",
            "96\n",
            "95\n",
            "94\n",
            "93\n",
            "92\n",
            "91\n",
            "90\n",
            "89\n",
            "88\n",
            "87\n",
            "86\n",
            "85\n",
            "84\n",
            "83\n",
            "82\n",
            "81\n",
            "80\n",
            "79\n",
            "78\n",
            "77\n",
            "76\n",
            "75\n",
            "74\n",
            "73\n",
            "72\n",
            "71\n",
            "70\n",
            "69\n",
            "68\n",
            "67\n",
            "66\n",
            "65\n",
            "64\n",
            "63\n",
            "62\n",
            "61\n",
            "60\n",
            "59\n",
            "58\n",
            "57\n",
            "56\n",
            "55\n",
            "54\n",
            "53\n",
            "52\n",
            "51\n",
            "50\n",
            "49\n",
            "48\n",
            "47\n",
            "46\n",
            "45\n",
            "44\n",
            "43\n",
            "42\n",
            "41\n",
            "40\n",
            "39\n",
            "38\n",
            "37\n",
            "36\n",
            "35\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4c1217783342>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-4c1217783342>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(model_config)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loss\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-6bcaf096ff2d>\u001b[0m in \u001b[0;36meval\u001b[0;34m(modelConfig)\u001b[0m\n\u001b[1;32m    156\u001b[0m         save_image(saveNoisy, os.path.join(\n\u001b[1;32m    157\u001b[0m             modelConfig[\"sampled_dir\"],  modelConfig[\"sampledNoisyImgName\"]), nrow=modelConfig[\"nrow\"])\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0msampledImgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisyImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0msampledImgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampledImgs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m  \u001b[0;31m# [0 ~ 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampledImgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-fc0fde1d362a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_T, labels)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mx_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nan in tensor.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mx_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install lpips"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vDrjC2PHDZvk",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1747101863919,
          "user_tz": 240,
          "elapsed": 2907,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "d5b06956-8f0c-4462-e3ff-3e9279eff30d"
      },
      "id": "vDrjC2PHDZvk",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lpips in /usr/local/lib/python3.10/dist-packages (0.1.4)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from lpips) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=0.4.0->lpips) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.2.1->lpips) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->lpips) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import NodeTransformer\n",
        "import os\n",
        "from typing import Dict\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from scipy import linalg, stats\n",
        "%pip install lpips\n",
        "import lpips\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision.models import inception_v3, Inception_V3_Weights\n",
        "from torch.nn import functional as F\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. METRIC FUNCTIONS\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load models\n",
        "weights = Inception_V3_Weights.DEFAULT\n",
        "_inception = inception_v3(weights=weights, aux_logits=True).to(device).eval()\n",
        "# and grab its recommended preprocessing transform:\n",
        "_incep_preprocess = weights.transforms()\n",
        "\n",
        "# _lpips = lpips.LPIPS(net='alex').to(device)\n",
        "_lpips = lpips.LPIPS(net='squeeze').to(device)\n",
        "\n",
        "def _get_inception_logits(x: torch.Tensor) -> torch.Tensor:\n",
        "    x_proc = _incep_preprocess(x)           # resize → crop → normalize\n",
        "    with torch.no_grad():\n",
        "      out = _inception(x_proc.to(device))\n",
        "    if isinstance(out, tuple):\n",
        "        out = out[0]                        # drop the aux_logits\n",
        "    return out\n",
        "\n",
        "def calculate_fid(mu1, sigma1, mu2, sigma2):\n",
        "    diff = mu1 - mu2\n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    covmean = covmean.real if np.iscomplexobj(covmean) else covmean\n",
        "    return diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
        "\n",
        "def compute_fid(real: torch.Tensor, fake: torch.Tensor) -> float:\n",
        "    act_real = _get_inception_logits(real).cpu().numpy()\n",
        "    act_fake = _get_inception_logits(fake).cpu().numpy()\n",
        "    mu_r, sigma_r = act_real.mean(0), np.cov(act_real, rowvar=False)\n",
        "    mu_f, sigma_f = act_fake.mean(0), np.cov(act_fake, rowvar=False)\n",
        "    return calculate_fid(mu_r, sigma_r, mu_f, sigma_f)\n",
        "\n",
        "def compute_inception_score(fake: torch.Tensor, splits=10):\n",
        "    preds = F.softmax(_get_inception_logits(fake), dim=1).cpu().numpy()\n",
        "    N = preds.shape[0]\n",
        "    scores = []\n",
        "    for k in range(splits):\n",
        "        part = preds[k*(N//splits):(k+1)*(N//splits)]\n",
        "        py = part.mean(axis=0)\n",
        "        scores.append(np.exp(np.mean([stats.entropy(p, py) for p in part])))\n",
        "    return float(np.mean(scores)), float(np.std(scores))\n",
        "\n",
        "def compute_lpips(real: torch.Tensor, fake: torch.Tensor) -> float:\n",
        "    # LPIPS expects input in [-1,+1]\n",
        "    r = 2*real - 1\n",
        "    f = 2*fake - 1\n",
        "    with torch.no_grad():\n",
        "        return float(_lpips(f.to(device), r.to(device)).mean())\n",
        "\n",
        "def evaluate_all(real: torch.Tensor, fake: torch.Tensor) -> Dict[str, float]:\n",
        "    return {\n",
        "        \"FID\": compute_fid(real, fake),\n",
        "        \"IS_mean\": compute_inception_score(fake)[0],\n",
        "        \"IS_std\":  compute_inception_score(fake)[1],\n",
        "        \"LPIPS\": compute_lpips(real, fake),\n",
        "    }\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. REAL CIFAR-10 LOADER\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def load_cifar10_test(num_samples: int):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      real_images: Tensor [num_samples,3,32,32] in [0,1]\n",
        "      real_labels: Tensor [num_samples]\n",
        "    \"\"\"\n",
        "    ds = CIFAR10(root=\"./data\", train=False, download=True,\n",
        "                 transform=transforms.ToTensor())\n",
        "    if num_samples < len(ds):\n",
        "        ds = Subset(ds, list(range(num_samples)))\n",
        "    loader = DataLoader(ds, batch_size=num_samples, shuffle=False)\n",
        "    _imgs, labs = next(iter(loader))\n",
        "    imgs = dct_batch(_imgs)  # compute DCT of images before\n",
        "    return imgs.to(device), labs.to(device)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. EXTENDED eval() WITH METRICS\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def eval_with_metrics(modelConfig: Dict):\n",
        "    device = torch.device(modelConfig[\"device\"])\n",
        "    B = modelConfig[\"batch_size\"]\n",
        "\n",
        "    # --- load real CIFAR-10 test images\n",
        "    real_images, real_labels = load_cifar10_test(num_samples=B)\n",
        "    print(f\"Loaded {real_images.shape[0]} real CIFAR-10 test images.\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # build label tensor [0..9] repeated\n",
        "        step = B // 10\n",
        "        label_list = []\n",
        "        for k in range(10):\n",
        "            label_list += [torch.full((step,), k, dtype=torch.long)]\n",
        "        labels = torch.cat(label_list, dim=0).to(device)\n",
        "        print(\"Labels tensor:\", labels.shape)\n",
        "\n",
        "        # instantiate and load weights\n",
        "        model = UNet(\n",
        "            T=modelConfig[\"T\"],\n",
        "            num_labels=10,\n",
        "            ch=modelConfig[\"channel\"],\n",
        "            ch_mult=modelConfig[\"channel_mult\"],\n",
        "            num_res_blocks=modelConfig[\"num_res_blocks\"],\n",
        "            dropout=modelConfig[\"dropout\"]\n",
        "        ).to(device)\n",
        "        ckpt = torch.load(\n",
        "            os.path.join(modelConfig[\"save_dir\"], modelConfig[\"test_load_weight\"]),\n",
        "            map_location=device\n",
        "        )\n",
        "        model.load_state_dict(ckpt)\n",
        "        model.eval()\n",
        "        print(\"Model loaded.\")\n",
        "\n",
        "        # sampler\n",
        "        sampler = GaussianDiffusionSampler(\n",
        "            model,\n",
        "            modelConfig[\"beta_1\"],\n",
        "            modelConfig[\"beta_T\"],\n",
        "            modelConfig[\"T\"],\n",
        "            w=modelConfig[\"w\"]\n",
        "        ).to(device)\n",
        "\n",
        "        # sample from N(0,I)\n",
        "        noisy = torch.randn(\n",
        "            B, 3, modelConfig[\"img_size\"], modelConfig[\"img_size\"],\n",
        "            device=device\n",
        "        )\n",
        "        sampled = sampler(noisy, labels)\n",
        "        sampled = sampled.clamp(-0.5, +0.5) + 0.5   # to [0,1]\n",
        "        print(f\"Generated {sampled.shape[0]} fake images.\")\n",
        "\n",
        "        # (optional) save a grid\n",
        "        save_image(\n",
        "            sampled,\n",
        "            os.path.join(modelConfig[\"sampled_dir\"], \"SampledGuidanceImgs_fourier_dct.png\"),\n",
        "            nrow=modelConfig[\"nrow\"]\n",
        "        )\n",
        "        back_to_normal = idct_batch(sampled)\n",
        "        save_image(\n",
        "            back_to_normal,\n",
        "            os.path.join(modelConfig[\"sampled_dir\"], \"SampledGuidanceImgs_fourier_nodct.png\"),\n",
        "            nrow=modelConfig[\"nrow\"]\n",
        "        )\n",
        "        norm_to_normal = back_to_normal * 0.5 + 0.5\n",
        "        save_image(\n",
        "            norm_to_normal,\n",
        "            os.path.join(modelConfig[\"sampled_dir\"], \"SampledGuidanceImgs_fourier_nodct_norm.png\"),\n",
        "            nrow=modelConfig[\"nrow\"]\n",
        "        )\n",
        "        # backnforth = idct_batch(dct_batch(real_images))* 0.5 + 0.5\n",
        "        # save_image(\n",
        "        #     backnforth,\n",
        "        #     os.path.join(modelConfig[\"sampled_dir\"], \"SampledGuidanceImgs_fourier_DCTandinv.png\"),\n",
        "        #     nrow=modelConfig[\"nrow\"]\n",
        "        # )\n",
        "\n",
        "\n",
        "    # --- compute metrics\n",
        "    metrics = evaluate_all(real_images, sampled)\n",
        "    print(\"=== Evaluation Metrics ===\")\n",
        "    for k,v in metrics.items():\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def main(model_config=None):\n",
        "    modelConfig = {\n",
        "        \"state\": \"eval\", # or eval\n",
        "        \"epoch\": 70,\n",
        "        \"batch_size\": 80,\n",
        "        \"T\": 500,\n",
        "        \"channel\": 96,\n",
        "        \"channel_mult\": [1, 2, 2, 2],\n",
        "        \"num_res_blocks\": 2,\n",
        "        \"dropout\": 0.15,\n",
        "        \"lr\": 1e-4,\n",
        "        \"multiplier\": 2.5,\n",
        "        \"beta_1\": 1e-4,\n",
        "        \"beta_T\": 0.028,\n",
        "        \"img_size\": 24,\n",
        "        \"grad_clip\": 1.,\n",
        "        \"device\": \"cuda:0\",\n",
        "        \"w\": 1.8,\n",
        "        \"save_dir\": \"./\",\n",
        "        \"training_load_weight\": None,\n",
        "        \"test_load_weight\": \"ckpt_19_.pt\",\n",
        "        \"sampled_dir\": \"./SampledImgs/\",\n",
        "        \"sampledNoisyImgName\": \"NoisyGuidenceImgs.png\",\n",
        "        \"sampledImgName\": \"SampledGuidenceImgs.png\",\n",
        "        \"nrow\": 8\n",
        "    }\n",
        "    if model_config is not None:\n",
        "        modelConfig = model_config\n",
        "    if modelConfig[\"state\"] == \"train\":\n",
        "        training_loss = train(modelConfig)  # new\n",
        "        print(training_loss)  # new\n",
        "    else:\n",
        "        metrics = eval_with_metrics(modelConfig)\n",
        "        print(metrics)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szRT928MCY1v",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1747102656131,
          "user_tz": 240,
          "elapsed": 43467,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "29a29d43-7241-4f84-f353-7e299a7cf0b8"
      },
      "id": "szRT928MCY1v",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lpips in /usr/local/lib/python3.10/dist-packages (0.1.4)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from lpips) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=0.4.0->lpips) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.2.1->lpips) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->lpips) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_1_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_1_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/squeezenet1_1-b8a52dc0.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_1-b8a52dc0.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [squeeze], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.73M/4.73M [00:00<00:00, 22.6MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/squeeze.pth\n",
            "Files already downloaded and verified\n",
            "Loaded 80 real CIFAR-10 test images.\n",
            "Labels tensor: torch.Size([80])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-8b319514d720>:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "499\n",
            "498\n",
            "497\n",
            "496\n",
            "495\n",
            "494\n",
            "493\n",
            "492\n",
            "491\n",
            "490\n",
            "489\n",
            "488\n",
            "487\n",
            "486\n",
            "485\n",
            "484\n",
            "483\n",
            "482\n",
            "481\n",
            "480\n",
            "479\n",
            "478\n",
            "477\n",
            "476\n",
            "475\n",
            "474\n",
            "473\n",
            "472\n",
            "471\n",
            "470\n",
            "469\n",
            "468\n",
            "467\n",
            "466\n",
            "465\n",
            "464\n",
            "463\n",
            "462\n",
            "461\n",
            "460\n",
            "459\n",
            "458\n",
            "457\n",
            "456\n",
            "455\n",
            "454\n",
            "453\n",
            "452\n",
            "451\n",
            "450\n",
            "449\n",
            "448\n",
            "447\n",
            "446\n",
            "445\n",
            "444\n",
            "443\n",
            "442\n",
            "441\n",
            "440\n",
            "439\n",
            "438\n",
            "437\n",
            "436\n",
            "435\n",
            "434\n",
            "433\n",
            "432\n",
            "431\n",
            "430\n",
            "429\n",
            "428\n",
            "427\n",
            "426\n",
            "425\n",
            "424\n",
            "423\n",
            "422\n",
            "421\n",
            "420\n",
            "419\n",
            "418\n",
            "417\n",
            "416\n",
            "415\n",
            "414\n",
            "413\n",
            "412\n",
            "411\n",
            "410\n",
            "409\n",
            "408\n",
            "407\n",
            "406\n",
            "405\n",
            "404\n",
            "403\n",
            "402\n",
            "401\n",
            "400\n",
            "399\n",
            "398\n",
            "397\n",
            "396\n",
            "395\n",
            "394\n",
            "393\n",
            "392\n",
            "391\n",
            "390\n",
            "389\n",
            "388\n",
            "387\n",
            "386\n",
            "385\n",
            "384\n",
            "383\n",
            "382\n",
            "381\n",
            "380\n",
            "379\n",
            "378\n",
            "377\n",
            "376\n",
            "375\n",
            "374\n",
            "373\n",
            "372\n",
            "371\n",
            "370\n",
            "369\n",
            "368\n",
            "367\n",
            "366\n",
            "365\n",
            "364\n",
            "363\n",
            "362\n",
            "361\n",
            "360\n",
            "359\n",
            "358\n",
            "357\n",
            "356\n",
            "355\n",
            "354\n",
            "353\n",
            "352\n",
            "351\n",
            "350\n",
            "349\n",
            "348\n",
            "347\n",
            "346\n",
            "345\n",
            "344\n",
            "343\n",
            "342\n",
            "341\n",
            "340\n",
            "339\n",
            "338\n",
            "337\n",
            "336\n",
            "335\n",
            "334\n",
            "333\n",
            "332\n",
            "331\n",
            "330\n",
            "329\n",
            "328\n",
            "327\n",
            "326\n",
            "325\n",
            "324\n",
            "323\n",
            "322\n",
            "321\n",
            "320\n",
            "319\n",
            "318\n",
            "317\n",
            "316\n",
            "315\n",
            "314\n",
            "313\n",
            "312\n",
            "311\n",
            "310\n",
            "309\n",
            "308\n",
            "307\n",
            "306\n",
            "305\n",
            "304\n",
            "303\n",
            "302\n",
            "301\n",
            "300\n",
            "299\n",
            "298\n",
            "297\n",
            "296\n",
            "295\n",
            "294\n",
            "293\n",
            "292\n",
            "291\n",
            "290\n",
            "289\n",
            "288\n",
            "287\n",
            "286\n",
            "285\n",
            "284\n",
            "283\n",
            "282\n",
            "281\n",
            "280\n",
            "279\n",
            "278\n",
            "277\n",
            "276\n",
            "275\n",
            "274\n",
            "273\n",
            "272\n",
            "271\n",
            "270\n",
            "269\n",
            "268\n",
            "267\n",
            "266\n",
            "265\n",
            "264\n",
            "263\n",
            "262\n",
            "261\n",
            "260\n",
            "259\n",
            "258\n",
            "257\n",
            "256\n",
            "255\n",
            "254\n",
            "253\n",
            "252\n",
            "251\n",
            "250\n",
            "249\n",
            "248\n",
            "247\n",
            "246\n",
            "245\n",
            "244\n",
            "243\n",
            "242\n",
            "241\n",
            "240\n",
            "239\n",
            "238\n",
            "237\n",
            "236\n",
            "235\n",
            "234\n",
            "233\n",
            "232\n",
            "231\n",
            "230\n",
            "229\n",
            "228\n",
            "227\n",
            "226\n",
            "225\n",
            "224\n",
            "223\n",
            "222\n",
            "221\n",
            "220\n",
            "219\n",
            "218\n",
            "217\n",
            "216\n",
            "215\n",
            "214\n",
            "213\n",
            "212\n",
            "211\n",
            "210\n",
            "209\n",
            "208\n",
            "207\n",
            "206\n",
            "205\n",
            "204\n",
            "203\n",
            "202\n",
            "201\n",
            "200\n",
            "199\n",
            "198\n",
            "197\n",
            "196\n",
            "195\n",
            "194\n",
            "193\n",
            "192\n",
            "191\n",
            "190\n",
            "189\n",
            "188\n",
            "187\n",
            "186\n",
            "185\n",
            "184\n",
            "183\n",
            "182\n",
            "181\n",
            "180\n",
            "179\n",
            "178\n",
            "177\n",
            "176\n",
            "175\n",
            "174\n",
            "173\n",
            "172\n",
            "171\n",
            "170\n",
            "169\n",
            "168\n",
            "167\n",
            "166\n",
            "165\n",
            "164\n",
            "163\n",
            "162\n",
            "161\n",
            "160\n",
            "159\n",
            "158\n",
            "157\n",
            "156\n",
            "155\n",
            "154\n",
            "153\n",
            "152\n",
            "151\n",
            "150\n",
            "149\n",
            "148\n",
            "147\n",
            "146\n",
            "145\n",
            "144\n",
            "143\n",
            "142\n",
            "141\n",
            "140\n",
            "139\n",
            "138\n",
            "137\n",
            "136\n",
            "135\n",
            "134\n",
            "133\n",
            "132\n",
            "131\n",
            "130\n",
            "129\n",
            "128\n",
            "127\n",
            "126\n",
            "125\n",
            "124\n",
            "123\n",
            "122\n",
            "121\n",
            "120\n",
            "119\n",
            "118\n",
            "117\n",
            "116\n",
            "115\n",
            "114\n",
            "113\n",
            "112\n",
            "111\n",
            "110\n",
            "109\n",
            "108\n",
            "107\n",
            "106\n",
            "105\n",
            "104\n",
            "103\n",
            "102\n",
            "101\n",
            "100\n",
            "99\n",
            "98\n",
            "97\n",
            "96\n",
            "95\n",
            "94\n",
            "93\n",
            "92\n",
            "91\n",
            "90\n",
            "89\n",
            "88\n",
            "87\n",
            "86\n",
            "85\n",
            "84\n",
            "83\n",
            "82\n",
            "81\n",
            "80\n",
            "79\n",
            "78\n",
            "77\n",
            "76\n",
            "75\n",
            "74\n",
            "73\n",
            "72\n",
            "71\n",
            "70\n",
            "69\n",
            "68\n",
            "67\n",
            "66\n",
            "65\n",
            "64\n",
            "63\n",
            "62\n",
            "61\n",
            "60\n",
            "59\n",
            "58\n",
            "57\n",
            "56\n",
            "55\n",
            "54\n",
            "53\n",
            "52\n",
            "51\n",
            "50\n",
            "49\n",
            "48\n",
            "47\n",
            "46\n",
            "45\n",
            "44\n",
            "43\n",
            "42\n",
            "41\n",
            "40\n",
            "39\n",
            "38\n",
            "37\n",
            "36\n",
            "35\n",
            "34\n",
            "33\n",
            "32\n",
            "31\n",
            "30\n",
            "29\n",
            "28\n",
            "27\n",
            "26\n",
            "25\n",
            "24\n",
            "23\n",
            "22\n",
            "21\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "0\n",
            "Generated 80 fake images.\n",
            "=== Evaluation Metrics ===\n",
            "FID: 463.9092\n",
            "IS_mean: 1.5203\n",
            "IS_std: 0.1376\n",
            "LPIPS: 0.4955\n",
            "{'FID': 463.9092183863703, 'IS_mean': 1.5203293561935425, 'IS_std': 0.1375882774591446, 'LPIPS': 0.4955354630947113}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### SANITY CHECK FOR IMAGE\n",
        "def load_cifar10_testt(num_samples: int):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      real_images: Tensor [num_samples,3,32,32] in [0,1]\n",
        "      real_labels: Tensor [num_samples]\n",
        "    \"\"\"\n",
        "    ds = CIFAR10(root=\"./data\", train=False, download=True,\n",
        "                 transform=transforms.ToTensor())\n",
        "    if num_samples < len(ds):\n",
        "        ds = Subset(ds, list(range(num_samples)))\n",
        "    loader = DataLoader(ds, batch_size=num_samples, shuffle=False)\n",
        "    _imgs, labs = next(iter(loader))\n",
        "    # imgs = dct_batch(_imgs)  # compute DCT of images before\n",
        "    return _imgs.to(device), labs.to(device)\n",
        "def sanity_check(modelConfig: Dict):\n",
        "    device = torch.device(modelConfig[\"device\"])\n",
        "    B = modelConfig[\"batch_size\"]\n",
        "\n",
        "    # --- load real CIFAR-10 test images\n",
        "    real_images, real_labels = load_cifar10_testt(num_samples=B)\n",
        "    save_image(\n",
        "        real_images *0.5 + 0.5,\n",
        "        os.path.join(\"./BASELINE_1.png\"),\n",
        "        nrow=modelConfig[\"nrow\"]\n",
        "    )\n",
        "    backnforth = idct_batch(dct_batch(real_images))* 0.5 + 0.5\n",
        "    backnforth_no_clamp = idct_batch(dct_batch(real_images))\n",
        "\n",
        "    save_image(\n",
        "        backnforth,\n",
        "        os.path.join(\"./SampledGuidanceImgs_fourier_DCTandinv.png\"),\n",
        "        nrow=modelConfig[\"nrow\"]\n",
        "    )\n",
        "    save_image(\n",
        "        backnforth,\n",
        "        os.path.join(\"./SampledGuidanceImgs_fourier_DCTandinv_no_clamp.png\"),\n",
        "        nrow=modelConfig[\"nrow\"]\n",
        "    )\n",
        "    save_image(\n",
        "        real_images,\n",
        "        os.path.join(\"./BASELINE.png\"),\n",
        "        nrow=modelConfig[\"nrow\"]\n",
        "    )\n",
        "modelConfig = {\n",
        "        \"state\": \"eval\", # or eval\n",
        "        \"epoch\": 70,\n",
        "        \"batch_size\": 80,\n",
        "        \"T\": 500,\n",
        "        \"channel\": 128,\n",
        "        \"channel_mult\": [1, 2, 2, 2],\n",
        "        \"num_res_blocks\": 2,\n",
        "        \"dropout\": 0.15,\n",
        "        \"lr\": 1e-4,\n",
        "        \"multiplier\": 2.5,\n",
        "        \"beta_1\": 1e-4,\n",
        "        \"beta_T\": 0.028,\n",
        "        \"img_size\": 32,\n",
        "        \"grad_clip\": 1.,\n",
        "        \"device\": \"cuda:0\",\n",
        "        \"w\": 1.8,\n",
        "        \"save_dir\": \"./CheckpointsCondition2/\",\n",
        "        \"training_load_weight\": None,\n",
        "        \"test_load_weight\": \"ckpt_69_.pt\",\n",
        "        \"sampled_dir\": \"./SampledImgs/\",\n",
        "        \"sampledNoisyImgName\": \"NoisyGuidenceImgs.png\",\n",
        "        \"sampledImgName\": \"SampledGuidenceImgs.png\",\n",
        "        \"nrow\": 8\n",
        "    }\n",
        "sanity_check(modelConfig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1mfyelFAIh6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1747097042011,
          "user_tz": 240,
          "elapsed": 1055,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "bf75d1e9-ec0e-4516-e490-6c1953e18af6"
      },
      "id": "k1mfyelFAIh6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VwLWEc27B6xf"
      },
      "id": "VwLWEc27B6xf",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "fourier_domain_training",
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}