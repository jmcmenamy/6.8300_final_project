{
  "cells": [
    {
      "cell_type": "code",
      "id": "ISxTNMDKP21UGSRF78ZuN8Kh",
      "metadata": {
        "tags": [],
        "id": "ISxTNMDKP21UGSRF78ZuN8Kh",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1747043094246,
          "user_tz": 240,
          "elapsed": 7480,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import math\n",
        "from telnetlib import PRAGMA_HEARTBEAT\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import init\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "\n",
        "import os\n",
        "from typing import Dict\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.utils import save_image"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GradualWarmupScheduler(_LRScheduler):\n",
        "    def __init__(self, optimizer, multiplier, warm_epoch, after_scheduler=None):\n",
        "        self.multiplier = multiplier\n",
        "        self.total_epoch = warm_epoch\n",
        "        self.after_scheduler = after_scheduler\n",
        "        self.finished = False\n",
        "        self.last_epoch = None\n",
        "        self.base_lrs = None\n",
        "        super().__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self.last_epoch > self.total_epoch:\n",
        "            if self.after_scheduler:\n",
        "                if not self.finished:\n",
        "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
        "                    self.finished = True\n",
        "                return self.after_scheduler.get_lr()\n",
        "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
        "        return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
        "\n",
        "\n",
        "    def step(self, epoch=None, metrics=None):\n",
        "        if self.finished and self.after_scheduler:\n",
        "            if epoch is None:\n",
        "                self.after_scheduler.step(None)\n",
        "            else:\n",
        "                self.after_scheduler.step(epoch - self.total_epoch)\n",
        "        else:\n",
        "            return super(GradualWarmupScheduler, self).step(epoch)"
      ],
      "metadata": {
        "id": "eW7ipofkwjwj",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1747043094246,
          "user_tz": 240,
          "elapsed": 3,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "eW7ipofkwjwj",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_connect(x, drop_ratio):\n",
        "    keep_ratio = 1.0 - drop_ratio\n",
        "    mask = torch.empty([x.shape[0], 1, 1, 1], dtype=x.dtype, device=x.device)\n",
        "    mask.bernoulli_(p=keep_ratio)\n",
        "    x.div_(keep_ratio)\n",
        "    x.mul_(mask)\n",
        "    return x\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "class SineAct(nn.Module):\n",
        "    def __init__(self, omega_0: float = 30.0):\n",
        "        super().__init__()\n",
        "        self.omega_0 = omega_0\n",
        "    def forward(self, x):\n",
        "        return torch.sin(self.omega_0 * x)\n",
        "\n",
        "class SineLayer(nn.Module):\n",
        "    # See paper sec. 3.2, final paragraph, and supplement Sec. 1.5 for discussion of omega_0.\n",
        "\n",
        "    # If is_first=True, omega_0 is a frequency factor which simply multiplies the activations before the\n",
        "    # nonlinearity. Different signals may require different omega_0 in the first layer - this is a\n",
        "    # hyperparameter.\n",
        "\n",
        "    # If is_first=False, then the weights will be divided by omega_0 so as to keep the magnitude of\n",
        "    # activations constant, but boost gradients to the weight matrix (see supplement Sec. 1.5)\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True,\n",
        "                 is_first=False, omega_0=30):\n",
        "        super().__init__()\n",
        "        self.omega_0 = omega_0\n",
        "        self.is_first = is_first\n",
        "\n",
        "        self.in_features = in_features\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        with torch.no_grad():\n",
        "            if self.is_first:\n",
        "                self.linear.weight.uniform_(-1 / self.in_features,\n",
        "                                             1 / self.in_features)\n",
        "            else:\n",
        "                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0,\n",
        "                                             np.sqrt(6 / self.in_features) / self.omega_0)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return torch.sin(self.omega_0 * self.linear(input))\n",
        "\n",
        "\n",
        "class TimeEmbedding(nn.Module):\n",
        "    def __init__(self, T, d_model, dim):\n",
        "        assert d_model % 2 == 0\n",
        "        super().__init__()\n",
        "        emb = torch.arange(0, d_model, step=2) / d_model * math.log(10000)\n",
        "        emb = torch.exp(-emb)\n",
        "        pos = torch.arange(T).float()\n",
        "        emb = pos[:, None] * emb[None, :]\n",
        "        assert list(emb.shape) == [T, d_model // 2]\n",
        "        emb = torch.stack([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
        "        assert list(emb.shape) == [T, d_model // 2, 2]\n",
        "        emb = emb.view(T, d_model)\n",
        "\n",
        "        self.timembedding = nn.Sequential(\n",
        "            nn.Embedding.from_pretrained(emb, freeze=False),\n",
        "            # use SIREN in first layer of this MLP\n",
        "            nn.Linear(d_model, dim),\n",
        "            SineLayer(\n",
        "                in_features=dim,   # same as the embedding dim\n",
        "                out_features=dim,      # proj size\n",
        "                bias=True,\n",
        "                is_first=True,         # first SIREN layer here\n",
        "                omega_0=30.0             # ω₀ hyperparameter\n",
        "            ),\n",
        "            # nn.Linear(dim, dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, t):\n",
        "        emb = self.timembedding(t)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class ConditionalEmbedding(nn.Module):\n",
        "    def __init__(self, num_labels, d_model, dim):\n",
        "        assert d_model % 2 == 0\n",
        "        super().__init__()\n",
        "        self.condEmbedding = nn.Sequential(\n",
        "            nn.Embedding(num_embeddings=num_labels + 1, embedding_dim=d_model, padding_idx=0),\n",
        "            nn.Linear(d_model, dim),\n",
        "            # first SIREN layer for condition MLP\n",
        "            SineLayer(\n",
        "                in_features=dim,\n",
        "                out_features=dim,\n",
        "                bias=True,\n",
        "                is_first=True,\n",
        "                omega_0=30.0\n",
        "            ),\n",
        "            # nn.Linear(dim, dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, t):\n",
        "        emb = self.condEmbedding(t)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class DownSample(nn.Module):\n",
        "    def __init__(self, in_ch):\n",
        "        super().__init__()\n",
        "        self.c1 = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1)\n",
        "        self.c2 = nn.Conv2d(in_ch, in_ch, 5, stride=2, padding=2)\n",
        "\n",
        "    def forward(self, x, temb, cemb):\n",
        "        x = self.c1(x) + self.c2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpSample(nn.Module):\n",
        "    def __init__(self, in_ch):\n",
        "        super().__init__()\n",
        "        self.c = nn.Conv2d(in_ch, in_ch, 3, stride=1, padding=1)\n",
        "        self.t = nn.ConvTranspose2d(in_ch, in_ch, 5, 2, 2, 1)\n",
        "\n",
        "    def forward(self, x, temb, cemb):\n",
        "        _, _, H, W = x.shape\n",
        "        x = self.t(x)\n",
        "        x = self.c(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    def __init__(self, in_ch):\n",
        "        super().__init__()\n",
        "        self.group_norm = nn.GroupNorm(32, in_ch)\n",
        "        self.proj_q = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
        "        self.proj_k = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
        "        self.proj_v = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
        "        self.proj = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        h = self.group_norm(x)\n",
        "        q = self.proj_q(h)\n",
        "        k = self.proj_k(h)\n",
        "        v = self.proj_v(h)\n",
        "\n",
        "        q = q.permute(0, 2, 3, 1).view(B, H * W, C)\n",
        "        k = k.view(B, C, H * W)\n",
        "        w = torch.bmm(q, k) * (int(C) ** (-0.5))\n",
        "        assert list(w.shape) == [B, H * W, H * W]\n",
        "        w = F.softmax(w, dim=-1)\n",
        "\n",
        "        v = v.permute(0, 2, 3, 1).view(B, H * W, C)\n",
        "        h = torch.bmm(w, v)\n",
        "        assert list(h.shape) == [B, H * W, C]\n",
        "        h = h.view(B, H, W, C).permute(0, 3, 1, 2)\n",
        "        h = self.proj(h)\n",
        "\n",
        "        return x + h\n",
        "\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, tdim, dropout, attn=True):\n",
        "        super().__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.GroupNorm(32, in_ch),\n",
        "            SineAct(omega_0=1.0),\n",
        "            nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1),\n",
        "        )\n",
        "        self.temb_proj = SineLayer(\n",
        "                in_features=tdim,\n",
        "                out_features=out_ch,   # keep dim the same before final head\n",
        "                bias=True,\n",
        "                is_first=False,      # not your very first SIREN layer\n",
        "                omega_0=1.0\n",
        "            )\n",
        "            # Swish(),\n",
        "            # nn.Linear(tdim, out_ch),\n",
        "        # )\n",
        "        self.cond_proj = SineLayer(\n",
        "                in_features=tdim,\n",
        "                out_features=out_ch,   # keep dim the same before final head\n",
        "                bias=True,\n",
        "                is_first=False,      # not your very first SIREN layer\n",
        "                omega_0=1.0\n",
        "            )\n",
        "            # Swish(),\n",
        "            # nn.Linear(tdim, out_ch),\n",
        "        # )\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.GroupNorm(32, out_ch),\n",
        "            SineAct(omega_0=1.0),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1),\n",
        "        )\n",
        "        if in_ch != out_ch:\n",
        "            self.shortcut = nn.Conv2d(in_ch, out_ch, 1, stride=1, padding=0)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "        if attn:\n",
        "            self.attn = AttnBlock(out_ch)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "\n",
        "\n",
        "    def forward(self, x, temb, labels):\n",
        "        h = self.block1(x)\n",
        "        h += self.temb_proj(temb)[:, :, None, None]\n",
        "        h += self.cond_proj(labels)[:, :, None, None]\n",
        "        h = self.block2(h)\n",
        "\n",
        "        h = h + self.shortcut(x)\n",
        "        h = self.attn(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, T, num_labels, ch, ch_mult, num_res_blocks, dropout):\n",
        "        super().__init__()\n",
        "        tdim = ch * 4\n",
        "        self.time_embedding = TimeEmbedding(T, ch, tdim)\n",
        "        self.cond_embedding = ConditionalEmbedding(num_labels, ch, tdim)\n",
        "        self.head = nn.Conv2d(3, ch, kernel_size=3, stride=1, padding=1)\n",
        "        self.downblocks = nn.ModuleList()\n",
        "        chs = [ch]  # record output channel when dowmsample for upsample\n",
        "        now_ch = ch\n",
        "        for i, mult in enumerate(ch_mult):\n",
        "            out_ch = ch * mult\n",
        "            for _ in range(num_res_blocks):\n",
        "                self.downblocks.append(ResBlock(in_ch=now_ch, out_ch=out_ch, tdim=tdim, dropout=dropout))\n",
        "                now_ch = out_ch\n",
        "                chs.append(now_ch)\n",
        "            if i != len(ch_mult) - 1:\n",
        "                self.downblocks.append(DownSample(now_ch))\n",
        "                chs.append(now_ch)\n",
        "\n",
        "        self.middleblocks = nn.ModuleList([\n",
        "            ResBlock(now_ch, now_ch, tdim, dropout, attn=True),\n",
        "            ResBlock(now_ch, now_ch, tdim, dropout, attn=False),\n",
        "        ])\n",
        "\n",
        "        self.upblocks = nn.ModuleList()\n",
        "        for i, mult in reversed(list(enumerate(ch_mult))):\n",
        "            out_ch = ch * mult\n",
        "            for _ in range(num_res_blocks + 1):\n",
        "                self.upblocks.append(ResBlock(in_ch=chs.pop() + now_ch, out_ch=out_ch, tdim=tdim, dropout=dropout, attn=False))\n",
        "                now_ch = out_ch\n",
        "            if i != 0:\n",
        "                self.upblocks.append(UpSample(now_ch))\n",
        "        assert len(chs) == 0\n",
        "\n",
        "        self.tail = nn.Sequential(\n",
        "            nn.GroupNorm(32, now_ch),\n",
        "            SineAct(omega_0=1.0),\n",
        "            nn.Conv2d(now_ch, 3, 3, stride=1, padding=1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, t, labels):\n",
        "        # Timestep embedding\n",
        "        temb = self.time_embedding(t)\n",
        "        cemb = self.cond_embedding(labels)\n",
        "        # Downsampling\n",
        "        h = self.head(x)\n",
        "        hs = [h]\n",
        "        for layer in self.downblocks:\n",
        "            h = layer(h, temb, cemb)\n",
        "            hs.append(h)\n",
        "        # Middle\n",
        "        for layer in self.middleblocks:\n",
        "            h = layer(h, temb, cemb)\n",
        "        # Upsampling\n",
        "        for layer in self.upblocks:\n",
        "            if isinstance(layer, ResBlock):\n",
        "                h = torch.cat([h, hs.pop()], dim=1)\n",
        "            h = layer(h, temb, cemb)\n",
        "        h = self.tail(h)\n",
        "\n",
        "        assert len(hs) == 0\n",
        "        return h"
      ],
      "metadata": {
        "id": "NkSGL-rSwla2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1747043094246,
          "user_tz": 240,
          "elapsed": 2,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "NkSGL-rSwla2",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract(v, t, x_shape):\n",
        "    \"\"\"\n",
        "    Extract some coefficients at specified timesteps, then reshape to\n",
        "    [batch_size, 1, 1, 1, 1, ...] for broadcasting purposes.\n",
        "    \"\"\"\n",
        "    device = t.device\n",
        "    out = torch.gather(v, index=t, dim=0).float().to(device)\n",
        "    return out.view([t.shape[0]] + [1] * (len(x_shape) - 1))\n",
        "\n",
        "\n",
        "class GaussianDiffusionTrainer(nn.Module):\n",
        "    def __init__(self, model, beta_1, beta_T, T):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.T = T\n",
        "\n",
        "        self.register_buffer(\n",
        "            'betas', torch.linspace(beta_1, beta_T, T).double())\n",
        "        alphas = 1. - self.betas\n",
        "        alphas_bar = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
        "        self.register_buffer(\n",
        "            'sqrt_alphas_bar', torch.sqrt(alphas_bar))\n",
        "        self.register_buffer(\n",
        "            'sqrt_one_minus_alphas_bar', torch.sqrt(1. - alphas_bar))\n",
        "\n",
        "    def forward(self, x_0, labels):\n",
        "        \"\"\"\n",
        "        Algorithm 1.\n",
        "        \"\"\"\n",
        "        t = torch.randint(self.T, size=(x_0.shape[0], ), device=x_0.device)\n",
        "        noise = torch.randn_like(x_0)\n",
        "        x_t =   extract(self.sqrt_alphas_bar, t, x_0.shape) * x_0 + \\\n",
        "                extract(self.sqrt_one_minus_alphas_bar, t, x_0.shape) * noise\n",
        "        loss = F.mse_loss(self.model(x_t, t, labels), noise, reduction='none')\n",
        "        return loss\n",
        "\n",
        "\n",
        "class GaussianDiffusionSampler(nn.Module):\n",
        "    def __init__(self, model, beta_1, beta_T, T, w = 0.):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.T = T\n",
        "        ### In the classifier free guidence paper, w is the key to control the gudience.\n",
        "        ### w = 0 and with label = 0 means no guidence.\n",
        "        ### w > 0 and label > 0 means guidence. Guidence would be stronger if w is bigger.\n",
        "        self.w = w\n",
        "\n",
        "        self.register_buffer('betas', torch.linspace(beta_1, beta_T, T).double())\n",
        "        alphas = 1. - self.betas\n",
        "        alphas_bar = torch.cumprod(alphas, dim=0)\n",
        "        alphas_bar_prev = F.pad(alphas_bar, [1, 0], value=1)[:T]\n",
        "        self.register_buffer('coeff1', torch.sqrt(1. / alphas))\n",
        "        self.register_buffer('coeff2', self.coeff1 * (1. - alphas) / torch.sqrt(1. - alphas_bar))\n",
        "        self.register_buffer('posterior_var', self.betas * (1. - alphas_bar_prev) / (1. - alphas_bar))\n",
        "\n",
        "    def predict_xt_prev_mean_from_eps(self, x_t, t, eps):\n",
        "        assert x_t.shape == eps.shape\n",
        "        return extract(self.coeff1, t, x_t.shape) * x_t - extract(self.coeff2, t, x_t.shape) * eps\n",
        "\n",
        "    def p_mean_variance(self, x_t, t, labels):\n",
        "        # below: only log_variance is used in the KL computations\n",
        "        var = torch.cat([self.posterior_var[1:2], self.betas[1:]])\n",
        "        var = extract(var, t, x_t.shape)\n",
        "        eps = self.model(x_t, t, labels)\n",
        "        nonEps = self.model(x_t, t, torch.zeros_like(labels).to(labels.device))\n",
        "        eps = (1. + self.w) * eps - self.w * nonEps\n",
        "        xt_prev_mean = self.predict_xt_prev_mean_from_eps(x_t, t, eps=eps)\n",
        "        return xt_prev_mean, var\n",
        "\n",
        "    def forward(self, x_T, labels):\n",
        "        \"\"\"\n",
        "        Algorithm 2.\n",
        "        \"\"\"\n",
        "        x_t = x_T\n",
        "        for time_step in reversed(range(self.T)):\n",
        "            print(time_step)\n",
        "            t = x_t.new_ones([x_T.shape[0], ], dtype=torch.long) * time_step\n",
        "            mean, var= self.p_mean_variance(x_t=x_t, t=t, labels=labels)\n",
        "            if time_step > 0:\n",
        "                noise = torch.randn_like(x_t)\n",
        "            else:\n",
        "                noise = 0\n",
        "            x_t = mean + torch.sqrt(var) * noise\n",
        "            assert torch.isnan(x_t).int().sum() == 0, \"nan in tensor.\"\n",
        "        x_0 = x_t\n",
        "        return torch.clip(x_0, -1, 1)"
      ],
      "metadata": {
        "id": "RdvTBLunwnZR",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1747043097034,
          "user_tz": 240,
          "elapsed": 325,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "RdvTBLunwnZR",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(modelConfig: Dict):\n",
        "    device = torch.device(modelConfig[\"device\"])\n",
        "    # dataset\n",
        "    dataset = CIFAR10(\n",
        "        root='./CIFAR10', train=True, download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ]))\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=modelConfig[\"batch_size\"], shuffle=True, num_workers=4, drop_last=True, pin_memory=True)\n",
        "\n",
        "    # model setup\n",
        "    net_model = UNet(T=modelConfig[\"T\"], num_labels=10, ch=modelConfig[\"channel\"], ch_mult=modelConfig[\"channel_mult\"],\n",
        "                     num_res_blocks=modelConfig[\"num_res_blocks\"], dropout=modelConfig[\"dropout\"]).to(device)\n",
        "    if modelConfig[\"training_load_weight\"] is not None:\n",
        "        net_model.load_state_dict(torch.load(os.path.join(\n",
        "            modelConfig[\"save_dir\"], modelConfig[\"training_load_weight\"]), map_location=device), strict=False)\n",
        "        print(\"Model weight load down.\")\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        net_model.parameters(), lr=modelConfig[\"lr\"], weight_decay=1e-4)\n",
        "    cosineScheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer=optimizer, T_max=modelConfig[\"epoch\"], eta_min=0, last_epoch=-1)\n",
        "    warmUpScheduler = GradualWarmupScheduler(optimizer=optimizer, multiplier=modelConfig[\"multiplier\"],\n",
        "                                             warm_epoch=modelConfig[\"epoch\"] // 10, after_scheduler=cosineScheduler)\n",
        "    trainer = GaussianDiffusionTrainer(\n",
        "        net_model, modelConfig[\"beta_1\"], modelConfig[\"beta_T\"], modelConfig[\"T\"]).to(device)\n",
        "\n",
        "    # start training\n",
        "    losses = []\n",
        "    lrs = []\n",
        "    for e in range(modelConfig[\"epoch\"]):\n",
        "        epoch_losses = []\n",
        "        epoch_lrs = []\n",
        "        with tqdm(dataloader, dynamic_ncols=True) as tqdmDataLoader:\n",
        "            for images, labels in tqdmDataLoader:\n",
        "                # train\n",
        "                b = images.shape[0]\n",
        "                optimizer.zero_grad()\n",
        "                x_0 = images.to(device)\n",
        "                labels = labels.to(device) + 1\n",
        "                if np.random.rand() < 0.1:\n",
        "                    labels = torch.zeros_like(labels).to(device)\n",
        "                loss = trainer(x_0, labels).sum() / b ** 2.\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    net_model.parameters(), modelConfig[\"grad_clip\"])\n",
        "                optimizer.step()\n",
        "                loss_item = loss.item()\n",
        "                lr = optimizer.state_dict()['param_groups'][0][\"lr\"]\n",
        "                epoch_lrs.append(lr)\n",
        "                epoch_losses.append(loss_item)\n",
        "                tqdmDataLoader.set_postfix(ordered_dict={\n",
        "                    \"epoch\": e,\n",
        "                    \"loss: \": loss_item,\n",
        "                    \"img shape: \": x_0.shape,\n",
        "                    \"LR\": lr\n",
        "                })\n",
        "            tqdmDataLoader.set_postfix(ordered_dict={\n",
        "                \"epoch\": e,\n",
        "                \"loss: \": np.mean(epoch_losses),\n",
        "                \"img shape: \": x_0.shape,\n",
        "                \"LR\": np.mean(epoch_lrs)\n",
        "            })\n",
        "        warmUpScheduler.step()\n",
        "        losses.append(np.mean(epoch_losses))\n",
        "        lrs.append(np.mean(epoch_lrs))\n",
        "        torch.save(net_model.state_dict(), os.path.join(\n",
        "            modelConfig[\"save_dir\"], 'ckpt_' + str(e) + \"_.pt\"))\n",
        "    print(losses)\n",
        "    print(lrs)\n",
        "    with open('./data.txt', 'w') as f:\n",
        "        f.write(f\"{losses}\\n{lrs}\")\n",
        "\n",
        "\n",
        "def eval(modelConfig: Dict):\n",
        "    device = torch.device(modelConfig[\"device\"])\n",
        "    # load model and evaluate\n",
        "    with torch.no_grad():\n",
        "        step = int(modelConfig[\"batch_size\"] // 10)\n",
        "        labelList = []\n",
        "        k = 0\n",
        "        for i in range(1, modelConfig[\"batch_size\"] + 1):\n",
        "            labelList.append(torch.ones(size=[1]).long() * k)\n",
        "            if i % step == 0:\n",
        "                if k < 10 - 1:\n",
        "                    k += 1\n",
        "        labels = torch.cat(labelList, dim=0).long().to(device) + 1\n",
        "        print(\"labels: \", labels)\n",
        "        model = UNet(T=modelConfig[\"T\"], num_labels=10, ch=modelConfig[\"channel\"], ch_mult=modelConfig[\"channel_mult\"],\n",
        "                     num_res_blocks=modelConfig[\"num_res_blocks\"], dropout=modelConfig[\"dropout\"]).to(device)\n",
        "        ckpt = torch.load(os.path.join(\n",
        "            modelConfig[\"save_dir\"], modelConfig[\"test_load_weight\"]), map_location=device)\n",
        "        model.load_state_dict(ckpt)\n",
        "        print(\"model load weight done.\")\n",
        "        model.eval()\n",
        "        sampler = GaussianDiffusionSampler(\n",
        "            model, modelConfig[\"beta_1\"], modelConfig[\"beta_T\"], modelConfig[\"T\"], w=modelConfig[\"w\"]).to(device)\n",
        "        # Sampled from standard normal distribution\n",
        "        noisyImage = torch.randn(\n",
        "            size=[modelConfig[\"batch_size\"], 3, modelConfig[\"img_size\"], modelConfig[\"img_size\"]], device=device)\n",
        "        saveNoisy = torch.clamp(noisyImage * 0.5 + 0.5, 0, 1)\n",
        "        save_image(saveNoisy, os.path.join(\n",
        "            modelConfig[\"sampled_dir\"],  modelConfig[\"sampledNoisyImgName\"]), nrow=modelConfig[\"nrow\"])\n",
        "        sampledImgs = sampler(noisyImage, labels)\n",
        "        sampledImgs = sampledImgs * 0.5 + 0.5  # [0 ~ 1]\n",
        "        print(sampledImgs)\n",
        "        save_image(sampledImgs, os.path.join(\n",
        "            modelConfig[\"sampled_dir\"],  modelConfig[\"sampledImgName\"]), nrow=modelConfig[\"nrow\"])"
      ],
      "metadata": {
        "id": "PMG_mfGDwpEi",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1747043099717,
          "user_tz": 240,
          "elapsed": 136,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "PMG_mfGDwpEi",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelConfig = {\n",
        "    \"state\": \"eval\", # or eval\n",
        "    \"epoch\": 70,\n",
        "    \"batch_size\": 80,\n",
        "    \"T\": 500,\n",
        "    \"channel\": 128,\n",
        "    \"channel_mult\": [1, 2, 2, 2],\n",
        "    \"num_res_blocks\": 2,\n",
        "    \"dropout\": 0.15,\n",
        "    \"lr\": 1e-4,\n",
        "    \"multiplier\": 2.5,\n",
        "    \"beta_1\": 1e-4,\n",
        "    \"beta_T\": 0.028,\n",
        "    \"img_size\": 32,\n",
        "    \"grad_clip\": 1.,\n",
        "    \"device\": \"cuda:0\",\n",
        "    \"w\": 1.8,\n",
        "    \"save_dir\": \"./CheckpointsCondition2/\",\n",
        "    \"training_load_weight\": None,\n",
        "    \"test_load_weight\": \"ckpt_69_.pt\",\n",
        "    \"sampled_dir\": \"./SampledImgs/\",\n",
        "    \"sampledNoisyImgName\": \"NoisyGuidenceImgs.png\",\n",
        "    \"sampledImgName\": \"SampledGuidenceImgs.png\",\n",
        "    \"nrow\": 8\n",
        "}\n",
        "if modelConfig[\"state\"] == \"train\":\n",
        "    train(modelConfig)\n",
        "else:\n",
        "    eval(modelConfig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwfmJ2AUwrVz",
        "outputId": "81f8758d-6d5f-49e7-b671-c8f44d265a10",
        "collapsed": true,
        "executionInfo": {
          "status": "ok",
          "timestamp": 1747062292438,
          "user_tz": 240,
          "elapsed": 104599,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "HwfmJ2AUwrVz",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels:  tensor([ 1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,\n",
            "         3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,\n",
            "         5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,\n",
            "         7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,\n",
            "        10, 10, 10, 10, 10, 10, 10, 10], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-6bbdb3b5f02c>:92: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(os.path.join(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model load weight done.\n",
            "499\n",
            "498\n",
            "497\n",
            "496\n",
            "495\n",
            "494\n",
            "493\n",
            "492\n",
            "491\n",
            "490\n",
            "489\n",
            "488\n",
            "487\n",
            "486\n",
            "485\n",
            "484\n",
            "483\n",
            "482\n",
            "481\n",
            "480\n",
            "479\n",
            "478\n",
            "477\n",
            "476\n",
            "475\n",
            "474\n",
            "473\n",
            "472\n",
            "471\n",
            "470\n",
            "469\n",
            "468\n",
            "467\n",
            "466\n",
            "465\n",
            "464\n",
            "463\n",
            "462\n",
            "461\n",
            "460\n",
            "459\n",
            "458\n",
            "457\n",
            "456\n",
            "455\n",
            "454\n",
            "453\n",
            "452\n",
            "451\n",
            "450\n",
            "449\n",
            "448\n",
            "447\n",
            "446\n",
            "445\n",
            "444\n",
            "443\n",
            "442\n",
            "441\n",
            "440\n",
            "439\n",
            "438\n",
            "437\n",
            "436\n",
            "435\n",
            "434\n",
            "433\n",
            "432\n",
            "431\n",
            "430\n",
            "429\n",
            "428\n",
            "427\n",
            "426\n",
            "425\n",
            "424\n",
            "423\n",
            "422\n",
            "421\n",
            "420\n",
            "419\n",
            "418\n",
            "417\n",
            "416\n",
            "415\n",
            "414\n",
            "413\n",
            "412\n",
            "411\n",
            "410\n",
            "409\n",
            "408\n",
            "407\n",
            "406\n",
            "405\n",
            "404\n",
            "403\n",
            "402\n",
            "401\n",
            "400\n",
            "399\n",
            "398\n",
            "397\n",
            "396\n",
            "395\n",
            "394\n",
            "393\n",
            "392\n",
            "391\n",
            "390\n",
            "389\n",
            "388\n",
            "387\n",
            "386\n",
            "385\n",
            "384\n",
            "383\n",
            "382\n",
            "381\n",
            "380\n",
            "379\n",
            "378\n",
            "377\n",
            "376\n",
            "375\n",
            "374\n",
            "373\n",
            "372\n",
            "371\n",
            "370\n",
            "369\n",
            "368\n",
            "367\n",
            "366\n",
            "365\n",
            "364\n",
            "363\n",
            "362\n",
            "361\n",
            "360\n",
            "359\n",
            "358\n",
            "357\n",
            "356\n",
            "355\n",
            "354\n",
            "353\n",
            "352\n",
            "351\n",
            "350\n",
            "349\n",
            "348\n",
            "347\n",
            "346\n",
            "345\n",
            "344\n",
            "343\n",
            "342\n",
            "341\n",
            "340\n",
            "339\n",
            "338\n",
            "337\n",
            "336\n",
            "335\n",
            "334\n",
            "333\n",
            "332\n",
            "331\n",
            "330\n",
            "329\n",
            "328\n",
            "327\n",
            "326\n",
            "325\n",
            "324\n",
            "323\n",
            "322\n",
            "321\n",
            "320\n",
            "319\n",
            "318\n",
            "317\n",
            "316\n",
            "315\n",
            "314\n",
            "313\n",
            "312\n",
            "311\n",
            "310\n",
            "309\n",
            "308\n",
            "307\n",
            "306\n",
            "305\n",
            "304\n",
            "303\n",
            "302\n",
            "301\n",
            "300\n",
            "299\n",
            "298\n",
            "297\n",
            "296\n",
            "295\n",
            "294\n",
            "293\n",
            "292\n",
            "291\n",
            "290\n",
            "289\n",
            "288\n",
            "287\n",
            "286\n",
            "285\n",
            "284\n",
            "283\n",
            "282\n",
            "281\n",
            "280\n",
            "279\n",
            "278\n",
            "277\n",
            "276\n",
            "275\n",
            "274\n",
            "273\n",
            "272\n",
            "271\n",
            "270\n",
            "269\n",
            "268\n",
            "267\n",
            "266\n",
            "265\n",
            "264\n",
            "263\n",
            "262\n",
            "261\n",
            "260\n",
            "259\n",
            "258\n",
            "257\n",
            "256\n",
            "255\n",
            "254\n",
            "253\n",
            "252\n",
            "251\n",
            "250\n",
            "249\n",
            "248\n",
            "247\n",
            "246\n",
            "245\n",
            "244\n",
            "243\n",
            "242\n",
            "241\n",
            "240\n",
            "239\n",
            "238\n",
            "237\n",
            "236\n",
            "235\n",
            "234\n",
            "233\n",
            "232\n",
            "231\n",
            "230\n",
            "229\n",
            "228\n",
            "227\n",
            "226\n",
            "225\n",
            "224\n",
            "223\n",
            "222\n",
            "221\n",
            "220\n",
            "219\n",
            "218\n",
            "217\n",
            "216\n",
            "215\n",
            "214\n",
            "213\n",
            "212\n",
            "211\n",
            "210\n",
            "209\n",
            "208\n",
            "207\n",
            "206\n",
            "205\n",
            "204\n",
            "203\n",
            "202\n",
            "201\n",
            "200\n",
            "199\n",
            "198\n",
            "197\n",
            "196\n",
            "195\n",
            "194\n",
            "193\n",
            "192\n",
            "191\n",
            "190\n",
            "189\n",
            "188\n",
            "187\n",
            "186\n",
            "185\n",
            "184\n",
            "183\n",
            "182\n",
            "181\n",
            "180\n",
            "179\n",
            "178\n",
            "177\n",
            "176\n",
            "175\n",
            "174\n",
            "173\n",
            "172\n",
            "171\n",
            "170\n",
            "169\n",
            "168\n",
            "167\n",
            "166\n",
            "165\n",
            "164\n",
            "163\n",
            "162\n",
            "161\n",
            "160\n",
            "159\n",
            "158\n",
            "157\n",
            "156\n",
            "155\n",
            "154\n",
            "153\n",
            "152\n",
            "151\n",
            "150\n",
            "149\n",
            "148\n",
            "147\n",
            "146\n",
            "145\n",
            "144\n",
            "143\n",
            "142\n",
            "141\n",
            "140\n",
            "139\n",
            "138\n",
            "137\n",
            "136\n",
            "135\n",
            "134\n",
            "133\n",
            "132\n",
            "131\n",
            "130\n",
            "129\n",
            "128\n",
            "127\n",
            "126\n",
            "125\n",
            "124\n",
            "123\n",
            "122\n",
            "121\n",
            "120\n",
            "119\n",
            "118\n",
            "117\n",
            "116\n",
            "115\n",
            "114\n",
            "113\n",
            "112\n",
            "111\n",
            "110\n",
            "109\n",
            "108\n",
            "107\n",
            "106\n",
            "105\n",
            "104\n",
            "103\n",
            "102\n",
            "101\n",
            "100\n",
            "99\n",
            "98\n",
            "97\n",
            "96\n",
            "95\n",
            "94\n",
            "93\n",
            "92\n",
            "91\n",
            "90\n",
            "89\n",
            "88\n",
            "87\n",
            "86\n",
            "85\n",
            "84\n",
            "83\n",
            "82\n",
            "81\n",
            "80\n",
            "79\n",
            "78\n",
            "77\n",
            "76\n",
            "75\n",
            "74\n",
            "73\n",
            "72\n",
            "71\n",
            "70\n",
            "69\n",
            "68\n",
            "67\n",
            "66\n",
            "65\n",
            "64\n",
            "63\n",
            "62\n",
            "61\n",
            "60\n",
            "59\n",
            "58\n",
            "57\n",
            "56\n",
            "55\n",
            "54\n",
            "53\n",
            "52\n",
            "51\n",
            "50\n",
            "49\n",
            "48\n",
            "47\n",
            "46\n",
            "45\n",
            "44\n",
            "43\n",
            "42\n",
            "41\n",
            "40\n",
            "39\n",
            "38\n",
            "37\n",
            "36\n",
            "35\n",
            "34\n",
            "33\n",
            "32\n",
            "31\n",
            "30\n",
            "29\n",
            "28\n",
            "27\n",
            "26\n",
            "25\n",
            "24\n",
            "23\n",
            "22\n",
            "21\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "0\n",
            "tensor([[[[0.3332, 0.3253, 0.3474,  ..., 0.2803, 0.2734, 0.2634],\n",
            "          [0.3691, 0.3662, 0.3685,  ..., 0.3294, 0.3066, 0.3075],\n",
            "          [0.3650, 0.3754, 0.3762,  ..., 0.3500, 0.3271, 0.3265],\n",
            "          ...,\n",
            "          [0.3436, 0.3127, 0.3113,  ..., 0.3455, 0.3104, 0.3218],\n",
            "          [0.2768, 0.2554, 0.2660,  ..., 0.2657, 0.2696, 0.3084],\n",
            "          [0.1897, 0.1877, 0.1827,  ..., 0.2320, 0.2580, 0.2872]],\n",
            "\n",
            "         [[0.6160, 0.6262, 0.6368,  ..., 0.6070, 0.5982, 0.5777],\n",
            "          [0.6551, 0.6618, 0.6689,  ..., 0.6483, 0.6266, 0.6308],\n",
            "          [0.6641, 0.6627, 0.6624,  ..., 0.6880, 0.6584, 0.6544],\n",
            "          ...,\n",
            "          [0.3403, 0.3117, 0.3153,  ..., 0.3041, 0.2726, 0.3048],\n",
            "          [0.2619, 0.2542, 0.2681,  ..., 0.2232, 0.2445, 0.2994],\n",
            "          [0.1647, 0.1580, 0.1602,  ..., 0.2210, 0.2341, 0.2508]],\n",
            "\n",
            "         [[0.8200, 0.8308, 0.8365,  ..., 0.8157, 0.8233, 0.8120],\n",
            "          [0.8618, 0.8598, 0.8659,  ..., 0.8483, 0.8343, 0.8285],\n",
            "          [0.8541, 0.8393, 0.8467,  ..., 0.8534, 0.8357, 0.8335],\n",
            "          ...,\n",
            "          [0.3820, 0.3629, 0.3729,  ..., 0.2710, 0.2044, 0.2294],\n",
            "          [0.3019, 0.2878, 0.3056,  ..., 0.2077, 0.1966, 0.2218],\n",
            "          [0.1798, 0.1697, 0.1720,  ..., 0.1884, 0.1913, 0.1778]]],\n",
            "\n",
            "\n",
            "        [[[0.9845, 0.9856, 0.9955,  ..., 0.9884, 0.9881, 0.9885],\n",
            "          [0.9876, 0.9907, 0.9973,  ..., 0.9900, 0.9914, 0.9960],\n",
            "          [0.9838, 0.9879, 0.9924,  ..., 0.9886, 0.9886, 0.9970],\n",
            "          ...,\n",
            "          [1.0000, 0.9976, 1.0000,  ..., 1.0000, 1.0000, 0.9970],\n",
            "          [0.9979, 0.9991, 1.0000,  ..., 1.0000, 1.0000, 0.9936],\n",
            "          [0.9997, 0.9953, 1.0000,  ..., 0.9987, 1.0000, 0.9988]],\n",
            "\n",
            "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [1.0000, 0.9942, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          ...,\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
            "\n",
            "         [[0.9985, 0.9972, 1.0000,  ..., 0.9887, 0.9941, 1.0000],\n",
            "          [1.0000, 1.0000, 0.9974,  ..., 1.0000, 0.9953, 0.9960],\n",
            "          [0.9976, 0.9954, 0.9999,  ..., 0.9913, 0.9940, 1.0000],\n",
            "          ...,\n",
            "          [1.0000, 0.9927, 0.9974,  ..., 0.9990, 1.0000, 0.9962],\n",
            "          [1.0000, 0.9920, 0.9966,  ..., 0.9964, 1.0000, 0.9969],\n",
            "          [1.0000, 0.9823, 0.9976,  ..., 1.0000, 1.0000, 1.0000]]],\n",
            "\n",
            "\n",
            "        [[[0.4297, 0.4196, 0.4605,  ..., 1.0000, 0.9860, 0.9229],\n",
            "          [0.6404, 0.7939, 0.8681,  ..., 0.9273, 0.9050, 0.8068],\n",
            "          [0.6749, 0.7529, 0.7861,  ..., 0.8274, 0.7971, 0.7317],\n",
            "          ...,\n",
            "          [0.4049, 0.4137, 0.4219,  ..., 0.3691, 0.2641, 0.2226],\n",
            "          [0.3678, 0.3901, 0.3912,  ..., 0.3199, 0.2759, 0.2197],\n",
            "          [0.3886, 0.3830, 0.3913,  ..., 0.2835, 0.2452, 0.2199]],\n",
            "\n",
            "         [[0.5220, 0.5291, 0.5920,  ..., 0.9419, 0.9326, 0.8852],\n",
            "          [0.6948, 0.8064, 0.8671,  ..., 0.8882, 0.8839, 0.8061],\n",
            "          [0.7390, 0.8081, 0.8272,  ..., 0.8175, 0.8039, 0.7629],\n",
            "          ...,\n",
            "          [0.4781, 0.4920, 0.5000,  ..., 0.4852, 0.3445, 0.2854],\n",
            "          [0.4541, 0.4743, 0.4766,  ..., 0.4090, 0.3518, 0.2946],\n",
            "          [0.4690, 0.4589, 0.4656,  ..., 0.3459, 0.3088, 0.2915]],\n",
            "\n",
            "         [[0.6581, 0.6510, 0.7319,  ..., 0.9385, 0.9378, 0.9140],\n",
            "          [0.7820, 0.8839, 0.9470,  ..., 0.9064, 0.9061, 0.8568],\n",
            "          [0.8178, 0.8844, 0.8987,  ..., 0.8568, 0.8563, 0.8431],\n",
            "          ...,\n",
            "          [0.5646, 0.5948, 0.5976,  ..., 0.5908, 0.3548, 0.1927],\n",
            "          [0.5479, 0.5748, 0.5713,  ..., 0.4759, 0.2976, 0.0759],\n",
            "          [0.5601, 0.5472, 0.5558,  ..., 0.3969, 0.2657, 0.0902]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.9844, 0.9854, 0.9795,  ..., 0.9765, 0.9702, 0.9514],\n",
            "          [0.9949, 0.9950, 0.9968,  ..., 0.9974, 0.9886, 0.9860],\n",
            "          [0.9913, 0.9859, 0.9825,  ..., 0.9896, 0.9949, 0.9880],\n",
            "          ...,\n",
            "          [0.2098, 0.1854, 0.1563,  ..., 0.4403, 0.4494, 0.4507],\n",
            "          [0.2517, 0.1508, 0.1008,  ..., 0.4503, 0.4542, 0.4526],\n",
            "          [0.2086, 0.1642, 0.1429,  ..., 0.4679, 0.4727, 0.4726]],\n",
            "\n",
            "         [[0.9669, 0.9687, 0.9650,  ..., 0.9641, 0.9466, 0.9577],\n",
            "          [0.9790, 0.9812, 0.9816,  ..., 0.9847, 0.9815, 0.9719],\n",
            "          [0.9774, 0.9770, 0.9738,  ..., 0.9865, 0.9804, 0.9702],\n",
            "          ...,\n",
            "          [0.1858, 0.1845, 0.1686,  ..., 0.4009, 0.4009, 0.4038],\n",
            "          [0.2287, 0.1470, 0.1115,  ..., 0.4111, 0.4134, 0.4177],\n",
            "          [0.1826, 0.1606, 0.1474,  ..., 0.4325, 0.4431, 0.4430]],\n",
            "\n",
            "         [[0.9899, 0.9835, 0.9774,  ..., 0.9645, 0.9562, 0.9550],\n",
            "          [0.9958, 0.9919, 0.9850,  ..., 0.9868, 0.9780, 0.9818],\n",
            "          [0.9845, 0.9848, 0.9762,  ..., 0.9840, 0.9826, 0.9768],\n",
            "          ...,\n",
            "          [0.2052, 0.1961, 0.1801,  ..., 0.4167, 0.4138, 0.4117],\n",
            "          [0.2383, 0.1610, 0.1240,  ..., 0.4266, 0.4194, 0.4222],\n",
            "          [0.2129, 0.1783, 0.1601,  ..., 0.4412, 0.4384, 0.4446]]],\n",
            "\n",
            "\n",
            "        [[[0.6988, 0.6730, 0.6711,  ..., 0.8203, 0.7688, 0.7452],\n",
            "          [0.7002, 0.6953, 0.6997,  ..., 0.8568, 0.7943, 0.7503],\n",
            "          [0.6660, 0.6538, 0.6675,  ..., 0.8768, 0.8114, 0.7575],\n",
            "          ...,\n",
            "          [0.5600, 0.4487, 0.3299,  ..., 0.2699, 0.3822, 0.5560],\n",
            "          [0.6862, 0.6779, 0.6086,  ..., 0.1822, 0.2390, 0.5277],\n",
            "          [0.7413, 0.7310, 0.7225,  ..., 0.5230, 0.5373, 0.6484]],\n",
            "\n",
            "         [[0.8031, 0.7781, 0.7785,  ..., 0.8995, 0.8552, 0.8432],\n",
            "          [0.8032, 0.7893, 0.7834,  ..., 0.9398, 0.8805, 0.8581],\n",
            "          [0.7814, 0.7674, 0.7715,  ..., 0.9565, 0.8942, 0.8529],\n",
            "          ...,\n",
            "          [0.6179, 0.5002, 0.3612,  ..., 0.2572, 0.3648, 0.5428],\n",
            "          [0.7110, 0.7137, 0.6405,  ..., 0.1630, 0.2125, 0.5077],\n",
            "          [0.7566, 0.7472, 0.7477,  ..., 0.4968, 0.5091, 0.6146]],\n",
            "\n",
            "         [[0.9626, 0.9439, 0.9462,  ..., 0.9715, 0.9383, 0.9510],\n",
            "          [0.9736, 0.9620, 0.9547,  ..., 1.0000, 0.9562, 0.9648],\n",
            "          [0.9416, 0.9488, 0.9425,  ..., 0.9946, 0.9627, 0.9575],\n",
            "          ...,\n",
            "          [0.6055, 0.5033, 0.3649,  ..., 0.2101, 0.3140, 0.4997],\n",
            "          [0.7131, 0.7215, 0.6450,  ..., 0.1192, 0.1643, 0.4565],\n",
            "          [0.7455, 0.7316, 0.7250,  ..., 0.3473, 0.3612, 0.4923]]],\n",
            "\n",
            "\n",
            "        [[[0.6904, 0.7000, 0.7251,  ..., 0.6530, 0.6514, 0.6452],\n",
            "          [0.6940, 0.7190, 0.7349,  ..., 0.6478, 0.6604, 0.6037],\n",
            "          [0.6815, 0.7056, 0.7283,  ..., 0.5777, 0.6215, 0.5614],\n",
            "          ...,\n",
            "          [0.6557, 0.6451, 0.6380,  ..., 0.5336, 0.5269, 0.5291],\n",
            "          [0.6013, 0.6110, 0.6347,  ..., 0.5177, 0.5133, 0.5098],\n",
            "          [0.6231, 0.6348, 0.6525,  ..., 0.4968, 0.4851, 0.4672]],\n",
            "\n",
            "         [[0.8906, 0.9259, 0.9377,  ..., 0.7995, 0.7761, 0.7552],\n",
            "          [0.8999, 0.9289, 0.9402,  ..., 0.7950, 0.7687, 0.6899],\n",
            "          [0.8955, 0.9291, 0.9349,  ..., 0.6989, 0.7133, 0.6422],\n",
            "          ...,\n",
            "          [0.6163, 0.6041, 0.5965,  ..., 0.5052, 0.5030, 0.5051],\n",
            "          [0.5705, 0.5730, 0.5982,  ..., 0.4929, 0.4891, 0.4866],\n",
            "          [0.5814, 0.5940, 0.6079,  ..., 0.4648, 0.4616, 0.4444]],\n",
            "\n",
            "         [[0.9642, 0.9827, 0.9974,  ..., 0.9029, 0.8608, 0.8311],\n",
            "          [0.9547, 0.9793, 0.9979,  ..., 0.8994, 0.8496, 0.7646],\n",
            "          [0.9508, 0.9948, 0.9963,  ..., 0.8045, 0.7770, 0.6913],\n",
            "          ...,\n",
            "          [0.5103, 0.5001, 0.4925,  ..., 0.4417, 0.4390, 0.4468],\n",
            "          [0.4694, 0.4686, 0.4867,  ..., 0.4264, 0.4248, 0.4242],\n",
            "          [0.4607, 0.4717, 0.5036,  ..., 0.3983, 0.4001, 0.3852]]]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install lpips"
      ],
      "metadata": {
        "id": "E1YnZ8w2w9eP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "executionInfo": {
          "status": "ok",
          "timestamp": 1747062773316,
          "user_tz": 240,
          "elapsed": 3067,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "92532551-2fe7-4514-d317-748dfa8f78ae"
      },
      "id": "E1YnZ8w2w9eP",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lpips\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from lpips) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=0.4.0->lpips) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.2.1->lpips) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->lpips) (3.0.2)\n",
            "Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lpips\n",
            "Successfully installed lpips-0.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Dict\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from scipy import linalg, stats\n",
        "import lpips\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision.models import inception_v3, Inception_V3_Weights\n",
        "from torch.nn import functional as F\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. METRIC FUNCTIONS\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load models\n",
        "weights = Inception_V3_Weights.DEFAULT\n",
        "_inception = inception_v3(weights=weights, aux_logits=True).to(device).eval()\n",
        "# and grab its recommended preprocessing transform:\n",
        "_incep_preprocess = weights.transforms()\n",
        "\n",
        "_lpips = lpips.LPIPS(net='alex').to(device)\n",
        "\n",
        "def _get_inception_logits(x: torch.Tensor) -> torch.Tensor:\n",
        "    x_proc = _incep_preprocess(x)           # resize → crop → normalize\n",
        "    with torch.no_grad():\n",
        "      out = _inception(x_proc.to(device))\n",
        "    if isinstance(out, tuple):\n",
        "        out = out[0]                        # drop the aux_logits\n",
        "    return out\n",
        "\n",
        "def calculate_fid(mu1, sigma1, mu2, sigma2):\n",
        "    diff = mu1 - mu2\n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    covmean = covmean.real if np.iscomplexobj(covmean) else covmean\n",
        "    return diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
        "\n",
        "def compute_fid(real: torch.Tensor, fake: torch.Tensor) -> float:\n",
        "    act_real = _get_inception_logits(real).cpu().numpy()\n",
        "    act_fake = _get_inception_logits(fake).cpu().numpy()\n",
        "    mu_r, sigma_r = act_real.mean(0), np.cov(act_real, rowvar=False)\n",
        "    mu_f, sigma_f = act_fake.mean(0), np.cov(act_fake, rowvar=False)\n",
        "    return calculate_fid(mu_r, sigma_r, mu_f, sigma_f)\n",
        "\n",
        "def compute_inception_score(fake: torch.Tensor, splits=10):\n",
        "    preds = F.softmax(_get_inception_logits(fake), dim=1).cpu().numpy()\n",
        "    N = preds.shape[0]\n",
        "    scores = []\n",
        "    for k in range(splits):\n",
        "        part = preds[k*(N//splits):(k+1)*(N//splits)]\n",
        "        py = part.mean(axis=0)\n",
        "        scores.append(np.exp(np.mean([stats.entropy(p, py) for p in part])))\n",
        "    return float(np.mean(scores)), float(np.std(scores))\n",
        "\n",
        "def compute_lpips(real: torch.Tensor, fake: torch.Tensor) -> float:\n",
        "    # LPIPS expects input in [-1,+1]\n",
        "    r = 2*real - 1\n",
        "    f = 2*fake - 1\n",
        "    with torch.no_grad():\n",
        "        return float(_lpips(f.to(device), r.to(device)).mean())\n",
        "\n",
        "def evaluate_all(real: torch.Tensor, fake: torch.Tensor) -> Dict[str, float]:\n",
        "    return {\n",
        "        \"FID\": compute_fid(real, fake),\n",
        "        \"IS_mean\": compute_inception_score(fake)[0],\n",
        "        \"IS_std\":  compute_inception_score(fake)[1],\n",
        "        \"LPIPS\": compute_lpips(real, fake),\n",
        "    }\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. REAL CIFAR-10 LOADER\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def load_cifar10_test(num_samples: int):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      real_images: Tensor [num_samples,3,32,32] in [0,1]\n",
        "      real_labels: Tensor [num_samples]\n",
        "    \"\"\"\n",
        "    ds = CIFAR10(root=\"./data\", train=False, download=True,\n",
        "                 transform=transforms.ToTensor())\n",
        "    if num_samples < len(ds):\n",
        "        ds = Subset(ds, list(range(num_samples)))\n",
        "    loader = DataLoader(ds, batch_size=num_samples, shuffle=False)\n",
        "    imgs, labs = next(iter(loader))\n",
        "    return imgs.to(device), labs.to(device)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. EXTENDED eval() WITH METRICS\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def eval_with_metrics(modelConfig: Dict):\n",
        "    device = torch.device(modelConfig[\"device\"])\n",
        "    B = modelConfig[\"batch_size\"]\n",
        "\n",
        "    # --- load real CIFAR-10 test images\n",
        "    real_images, real_labels = load_cifar10_test(num_samples=B)\n",
        "    print(f\"Loaded {real_images.shape[0]} real CIFAR-10 test images.\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # build label tensor [0..9] repeated\n",
        "        step = B // 10\n",
        "        label_list = []\n",
        "        for k in range(10):\n",
        "            label_list += [torch.full((step,), k, dtype=torch.long)]\n",
        "        labels = torch.cat(label_list, dim=0).to(device)\n",
        "        print(\"Labels tensor:\", labels.shape)\n",
        "\n",
        "        # instantiate and load weights\n",
        "        model = UNet(\n",
        "            T=modelConfig[\"T\"],\n",
        "            num_labels=10,\n",
        "            ch=modelConfig[\"channel\"],\n",
        "            ch_mult=modelConfig[\"channel_mult\"],\n",
        "            num_res_blocks=modelConfig[\"num_res_blocks\"],\n",
        "            dropout=modelConfig[\"dropout\"]\n",
        "        ).to(device)\n",
        "        ckpt = torch.load(\n",
        "            os.path.join(modelConfig[\"save_dir\"], modelConfig[\"test_load_weight\"]),\n",
        "            map_location=device\n",
        "        )\n",
        "        model.load_state_dict(ckpt)\n",
        "        model.eval()\n",
        "        print(\"Model loaded.\")\n",
        "\n",
        "        # sampler\n",
        "        sampler = GaussianDiffusionSampler(\n",
        "            model,\n",
        "            modelConfig[\"beta_1\"],\n",
        "            modelConfig[\"beta_T\"],\n",
        "            modelConfig[\"T\"],\n",
        "            w=modelConfig[\"w\"]\n",
        "        ).to(device)\n",
        "\n",
        "        # sample from N(0,I)\n",
        "        noisy = torch.randn(\n",
        "            B, 3, modelConfig[\"img_size\"], modelConfig[\"img_size\"],\n",
        "            device=device\n",
        "        )\n",
        "        sampled = sampler(noisy, labels)\n",
        "        sampled = sampled.clamp(-0.5, +0.5) + 0.5   # to [0,1]\n",
        "        print(f\"Generated {sampled.shape[0]} fake images.\")\n",
        "\n",
        "        # (optional) save a grid\n",
        "        save_image(\n",
        "            sampled,\n",
        "            os.path.join(modelConfig[\"sampled_dir\"], modelConfig[\"sampledImgName\"]),\n",
        "            nrow=modelConfig[\"nrow\"]\n",
        "        )\n",
        "\n",
        "    # --- compute metrics\n",
        "    metrics = evaluate_all(real_images, sampled)\n",
        "    print(\"=== Evaluation Metrics ===\")\n",
        "    for k,v in metrics.items():\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def main(model_config=None):\n",
        "    modelConfig = {\n",
        "        \"state\": \"eval\", # or eval\n",
        "        \"epoch\": 70,\n",
        "        \"batch_size\": 80,\n",
        "        \"T\": 500,\n",
        "        \"channel\": 128,\n",
        "        \"channel_mult\": [1, 2, 2, 2],\n",
        "        \"num_res_blocks\": 2,\n",
        "        \"dropout\": 0.15,\n",
        "        \"lr\": 1e-4,\n",
        "        \"multiplier\": 2.5,\n",
        "        \"beta_1\": 1e-4,\n",
        "        \"beta_T\": 0.028,\n",
        "        \"img_size\": 32,\n",
        "        \"grad_clip\": 1.,\n",
        "        \"device\": \"cuda:0\",\n",
        "        \"w\": 1.8,\n",
        "        \"save_dir\": \"./CheckpointsCondition2/\",\n",
        "        \"training_load_weight\": None,\n",
        "        \"test_load_weight\": \"ckpt_69_.pt\",\n",
        "        \"sampled_dir\": \"./SampledImgs/\",\n",
        "        \"sampledNoisyImgName\": \"NoisyGuidenceImgs.png\",\n",
        "        \"sampledImgName\": \"SampledGuidenceImgs.png\",\n",
        "        \"nrow\": 8\n",
        "    }\n",
        "    if model_config is not None:\n",
        "        modelConfig = model_config\n",
        "    if modelConfig[\"state\"] == \"train\":\n",
        "        training_loss = train(modelConfig)  # new\n",
        "        print(training_loss)  # new\n",
        "    else:\n",
        "        metrics = eval_with_metrics(modelConfig)\n",
        "        print(metrics)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cq-0QUXwDkOA",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1747064382525,
          "user_tz": 240,
          "elapsed": 116026,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "5a4e5683-5c10-43f4-ef53-00b4db8330a0"
      },
      "id": "Cq-0QUXwDkOA",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
            "100%|██████████| 104M/104M [00:00<00:00, 183MB/s] \n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:01<00:00, 210MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 80.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Loaded 80 real CIFAR-10 test images.\n",
            "Labels tensor: torch.Size([80])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-d74e317f3c3e>:124: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "499\n",
            "498\n",
            "497\n",
            "496\n",
            "495\n",
            "494\n",
            "493\n",
            "492\n",
            "491\n",
            "490\n",
            "489\n",
            "488\n",
            "487\n",
            "486\n",
            "485\n",
            "484\n",
            "483\n",
            "482\n",
            "481\n",
            "480\n",
            "479\n",
            "478\n",
            "477\n",
            "476\n",
            "475\n",
            "474\n",
            "473\n",
            "472\n",
            "471\n",
            "470\n",
            "469\n",
            "468\n",
            "467\n",
            "466\n",
            "465\n",
            "464\n",
            "463\n",
            "462\n",
            "461\n",
            "460\n",
            "459\n",
            "458\n",
            "457\n",
            "456\n",
            "455\n",
            "454\n",
            "453\n",
            "452\n",
            "451\n",
            "450\n",
            "449\n",
            "448\n",
            "447\n",
            "446\n",
            "445\n",
            "444\n",
            "443\n",
            "442\n",
            "441\n",
            "440\n",
            "439\n",
            "438\n",
            "437\n",
            "436\n",
            "435\n",
            "434\n",
            "433\n",
            "432\n",
            "431\n",
            "430\n",
            "429\n",
            "428\n",
            "427\n",
            "426\n",
            "425\n",
            "424\n",
            "423\n",
            "422\n",
            "421\n",
            "420\n",
            "419\n",
            "418\n",
            "417\n",
            "416\n",
            "415\n",
            "414\n",
            "413\n",
            "412\n",
            "411\n",
            "410\n",
            "409\n",
            "408\n",
            "407\n",
            "406\n",
            "405\n",
            "404\n",
            "403\n",
            "402\n",
            "401\n",
            "400\n",
            "399\n",
            "398\n",
            "397\n",
            "396\n",
            "395\n",
            "394\n",
            "393\n",
            "392\n",
            "391\n",
            "390\n",
            "389\n",
            "388\n",
            "387\n",
            "386\n",
            "385\n",
            "384\n",
            "383\n",
            "382\n",
            "381\n",
            "380\n",
            "379\n",
            "378\n",
            "377\n",
            "376\n",
            "375\n",
            "374\n",
            "373\n",
            "372\n",
            "371\n",
            "370\n",
            "369\n",
            "368\n",
            "367\n",
            "366\n",
            "365\n",
            "364\n",
            "363\n",
            "362\n",
            "361\n",
            "360\n",
            "359\n",
            "358\n",
            "357\n",
            "356\n",
            "355\n",
            "354\n",
            "353\n",
            "352\n",
            "351\n",
            "350\n",
            "349\n",
            "348\n",
            "347\n",
            "346\n",
            "345\n",
            "344\n",
            "343\n",
            "342\n",
            "341\n",
            "340\n",
            "339\n",
            "338\n",
            "337\n",
            "336\n",
            "335\n",
            "334\n",
            "333\n",
            "332\n",
            "331\n",
            "330\n",
            "329\n",
            "328\n",
            "327\n",
            "326\n",
            "325\n",
            "324\n",
            "323\n",
            "322\n",
            "321\n",
            "320\n",
            "319\n",
            "318\n",
            "317\n",
            "316\n",
            "315\n",
            "314\n",
            "313\n",
            "312\n",
            "311\n",
            "310\n",
            "309\n",
            "308\n",
            "307\n",
            "306\n",
            "305\n",
            "304\n",
            "303\n",
            "302\n",
            "301\n",
            "300\n",
            "299\n",
            "298\n",
            "297\n",
            "296\n",
            "295\n",
            "294\n",
            "293\n",
            "292\n",
            "291\n",
            "290\n",
            "289\n",
            "288\n",
            "287\n",
            "286\n",
            "285\n",
            "284\n",
            "283\n",
            "282\n",
            "281\n",
            "280\n",
            "279\n",
            "278\n",
            "277\n",
            "276\n",
            "275\n",
            "274\n",
            "273\n",
            "272\n",
            "271\n",
            "270\n",
            "269\n",
            "268\n",
            "267\n",
            "266\n",
            "265\n",
            "264\n",
            "263\n",
            "262\n",
            "261\n",
            "260\n",
            "259\n",
            "258\n",
            "257\n",
            "256\n",
            "255\n",
            "254\n",
            "253\n",
            "252\n",
            "251\n",
            "250\n",
            "249\n",
            "248\n",
            "247\n",
            "246\n",
            "245\n",
            "244\n",
            "243\n",
            "242\n",
            "241\n",
            "240\n",
            "239\n",
            "238\n",
            "237\n",
            "236\n",
            "235\n",
            "234\n",
            "233\n",
            "232\n",
            "231\n",
            "230\n",
            "229\n",
            "228\n",
            "227\n",
            "226\n",
            "225\n",
            "224\n",
            "223\n",
            "222\n",
            "221\n",
            "220\n",
            "219\n",
            "218\n",
            "217\n",
            "216\n",
            "215\n",
            "214\n",
            "213\n",
            "212\n",
            "211\n",
            "210\n",
            "209\n",
            "208\n",
            "207\n",
            "206\n",
            "205\n",
            "204\n",
            "203\n",
            "202\n",
            "201\n",
            "200\n",
            "199\n",
            "198\n",
            "197\n",
            "196\n",
            "195\n",
            "194\n",
            "193\n",
            "192\n",
            "191\n",
            "190\n",
            "189\n",
            "188\n",
            "187\n",
            "186\n",
            "185\n",
            "184\n",
            "183\n",
            "182\n",
            "181\n",
            "180\n",
            "179\n",
            "178\n",
            "177\n",
            "176\n",
            "175\n",
            "174\n",
            "173\n",
            "172\n",
            "171\n",
            "170\n",
            "169\n",
            "168\n",
            "167\n",
            "166\n",
            "165\n",
            "164\n",
            "163\n",
            "162\n",
            "161\n",
            "160\n",
            "159\n",
            "158\n",
            "157\n",
            "156\n",
            "155\n",
            "154\n",
            "153\n",
            "152\n",
            "151\n",
            "150\n",
            "149\n",
            "148\n",
            "147\n",
            "146\n",
            "145\n",
            "144\n",
            "143\n",
            "142\n",
            "141\n",
            "140\n",
            "139\n",
            "138\n",
            "137\n",
            "136\n",
            "135\n",
            "134\n",
            "133\n",
            "132\n",
            "131\n",
            "130\n",
            "129\n",
            "128\n",
            "127\n",
            "126\n",
            "125\n",
            "124\n",
            "123\n",
            "122\n",
            "121\n",
            "120\n",
            "119\n",
            "118\n",
            "117\n",
            "116\n",
            "115\n",
            "114\n",
            "113\n",
            "112\n",
            "111\n",
            "110\n",
            "109\n",
            "108\n",
            "107\n",
            "106\n",
            "105\n",
            "104\n",
            "103\n",
            "102\n",
            "101\n",
            "100\n",
            "99\n",
            "98\n",
            "97\n",
            "96\n",
            "95\n",
            "94\n",
            "93\n",
            "92\n",
            "91\n",
            "90\n",
            "89\n",
            "88\n",
            "87\n",
            "86\n",
            "85\n",
            "84\n",
            "83\n",
            "82\n",
            "81\n",
            "80\n",
            "79\n",
            "78\n",
            "77\n",
            "76\n",
            "75\n",
            "74\n",
            "73\n",
            "72\n",
            "71\n",
            "70\n",
            "69\n",
            "68\n",
            "67\n",
            "66\n",
            "65\n",
            "64\n",
            "63\n",
            "62\n",
            "61\n",
            "60\n",
            "59\n",
            "58\n",
            "57\n",
            "56\n",
            "55\n",
            "54\n",
            "53\n",
            "52\n",
            "51\n",
            "50\n",
            "49\n",
            "48\n",
            "47\n",
            "46\n",
            "45\n",
            "44\n",
            "43\n",
            "42\n",
            "41\n",
            "40\n",
            "39\n",
            "38\n",
            "37\n",
            "36\n",
            "35\n",
            "34\n",
            "33\n",
            "32\n",
            "31\n",
            "30\n",
            "29\n",
            "28\n",
            "27\n",
            "26\n",
            "25\n",
            "24\n",
            "23\n",
            "22\n",
            "21\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "0\n",
            "Generated 80 fake images.\n",
            "=== Evaluation Metrics ===\n",
            "FID: 699.4562\n",
            "IS_mean: 2.4969\n",
            "IS_std: 0.3515\n",
            "LPIPS: 0.2404\n",
            "{'FID': 699.456205009933, 'IS_mean': 2.4968581199645996, 'IS_std': 0.35147786140441895, 'LPIPS': 0.24035027623176575}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "siren",
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}