{
  "cells": [
    {
      "cell_type": "code",
      "id": "uKgIbjPGVDHpVgOUJkpXl7Tw",
      "metadata": {
        "tags": [],
        "id": "uKgIbjPGVDHpVgOUJkpXl7Tw",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1747043126301,
          "user_tz": 240,
          "elapsed": 7568,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import math\n",
        "from telnetlib import PRAGMA_HEARTBEAT\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import init\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "\n",
        "import os\n",
        "from typing import Dict\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.utils import save_image"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GradualWarmupScheduler(_LRScheduler):\n",
        "    def __init__(self, optimizer, multiplier, warm_epoch, after_scheduler=None):\n",
        "        self.multiplier = multiplier\n",
        "        self.total_epoch = warm_epoch\n",
        "        self.after_scheduler = after_scheduler\n",
        "        self.finished = False\n",
        "        self.last_epoch = None\n",
        "        self.base_lrs = None\n",
        "        super().__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self.last_epoch > self.total_epoch:\n",
        "            if self.after_scheduler:\n",
        "                if not self.finished:\n",
        "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
        "                    self.finished = True\n",
        "                return self.after_scheduler.get_lr()\n",
        "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
        "        return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
        "\n",
        "\n",
        "    def step(self, epoch=None, metrics=None):\n",
        "        if self.finished and self.after_scheduler:\n",
        "            if epoch is None:\n",
        "                self.after_scheduler.step(None)\n",
        "            else:\n",
        "                self.after_scheduler.step(epoch - self.total_epoch)\n",
        "        else:\n",
        "            return super(GradualWarmupScheduler, self).step(epoch)"
      ],
      "metadata": {
        "id": "h2FdsP2UyCxk",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1747043128903,
          "user_tz": 240,
          "elapsed": 201,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "h2FdsP2UyCxk",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_connect(x, drop_ratio):\n",
        "    keep_ratio = 1.0 - drop_ratio\n",
        "    mask = torch.empty([x.shape[0], 1, 1, 1], dtype=x.dtype, device=x.device)\n",
        "    mask.bernoulli_(p=keep_ratio)\n",
        "    x.div_(keep_ratio)\n",
        "    x.mul_(mask)\n",
        "    return x\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "class SineAct(nn.Module):\n",
        "    def __init__(self, omega_0: float = 30.0):\n",
        "        super().__init__()\n",
        "        self.omega_0 = omega_0\n",
        "    def forward(self, x):\n",
        "        return torch.sin(self.omega_0 * x)\n",
        "\n",
        "class SineLayer(nn.Module):\n",
        "    # See paper sec. 3.2, final paragraph, and supplement Sec. 1.5 for discussion of omega_0.\n",
        "\n",
        "    # If is_first=True, omega_0 is a frequency factor which simply multiplies the activations before the\n",
        "    # nonlinearity. Different signals may require different omega_0 in the first layer - this is a\n",
        "    # hyperparameter.\n",
        "\n",
        "    # If is_first=False, then the weights will be divided by omega_0 so as to keep the magnitude of\n",
        "    # activations constant, but boost gradients to the weight matrix (see supplement Sec. 1.5)\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True,\n",
        "                 is_first=False, omega_0=30):\n",
        "        super().__init__()\n",
        "        self.omega_0 = omega_0\n",
        "        self.is_first = is_first\n",
        "\n",
        "        self.in_features = in_features\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        with torch.no_grad():\n",
        "            if self.is_first:\n",
        "                self.linear.weight.uniform_(-1 / self.in_features,\n",
        "                                             1 / self.in_features)\n",
        "            else:\n",
        "                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0,\n",
        "                                             np.sqrt(6 / self.in_features) / self.omega_0)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return torch.sin(self.omega_0 * self.linear(input))\n",
        "\n",
        "\n",
        "class TimeEmbedding(nn.Module):\n",
        "    def __init__(self, T, d_model, dim):\n",
        "        assert d_model % 2 == 0\n",
        "        super().__init__()\n",
        "        emb = torch.arange(0, d_model, step=2) / d_model * math.log(10000)\n",
        "        emb = torch.exp(-emb)\n",
        "        pos = torch.arange(T).float()\n",
        "        emb = pos[:, None] * emb[None, :]\n",
        "        assert list(emb.shape) == [T, d_model // 2]\n",
        "        emb = torch.stack([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
        "        assert list(emb.shape) == [T, d_model // 2, 2]\n",
        "        emb = emb.view(T, d_model)\n",
        "\n",
        "        self.timembedding = nn.Sequential(\n",
        "            nn.Embedding.from_pretrained(emb, freeze=False),\n",
        "            # use SIREN in first layer of this MLP\n",
        "            nn.Linear(d_model, dim),\n",
        "            SineLayer(\n",
        "                in_features=dim,   # same as the embedding dim\n",
        "                out_features=dim,      # proj size\n",
        "                bias=True,\n",
        "                is_first=True,         # first SIREN layer here\n",
        "                omega_0=30.0             # ω₀ hyperparameter\n",
        "            ),\n",
        "            # nn.Linear(dim, dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, t):\n",
        "        emb = self.timembedding(t)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class ConditionalEmbedding(nn.Module):\n",
        "    def __init__(self, num_labels, d_model, dim):\n",
        "        assert d_model % 2 == 0\n",
        "        super().__init__()\n",
        "        self.condEmbedding = nn.Sequential(\n",
        "            nn.Embedding(num_embeddings=num_labels + 1, embedding_dim=d_model, padding_idx=0),\n",
        "            nn.Linear(d_model, dim),\n",
        "            # first SIREN layer for condition MLP\n",
        "            SineLayer(\n",
        "                in_features=dim,\n",
        "                out_features=dim,\n",
        "                bias=True,\n",
        "                is_first=True,\n",
        "                omega_0=30.0\n",
        "            ),\n",
        "            # nn.Linear(dim, dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, t):\n",
        "        emb = self.condEmbedding(t)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class DownSample(nn.Module):\n",
        "    def __init__(self, in_ch):\n",
        "        super().__init__()\n",
        "        self.c1 = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1)\n",
        "        self.c2 = nn.Conv2d(in_ch, in_ch, 5, stride=2, padding=2)\n",
        "\n",
        "    def forward(self, x, temb, cemb):\n",
        "        x = self.c1(x) + self.c2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpSample(nn.Module):\n",
        "    def __init__(self, in_ch):\n",
        "        super().__init__()\n",
        "        self.c = nn.Conv2d(in_ch, in_ch, 3, stride=1, padding=1)\n",
        "        self.t = nn.ConvTranspose2d(in_ch, in_ch, 5, 2, 2, 1)\n",
        "\n",
        "    def forward(self, x, temb, cemb):\n",
        "        _, _, H, W = x.shape\n",
        "        x = self.t(x)\n",
        "        x = self.c(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    def __init__(self, in_ch):\n",
        "        super().__init__()\n",
        "        self.group_norm = nn.GroupNorm(32, in_ch)\n",
        "        self.proj_q = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
        "        self.proj_k = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
        "        self.proj_v = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
        "        self.proj = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        h = self.group_norm(x)\n",
        "        q = self.proj_q(h)\n",
        "        k = self.proj_k(h)\n",
        "        v = self.proj_v(h)\n",
        "\n",
        "        q = q.permute(0, 2, 3, 1).view(B, H * W, C)\n",
        "        k = k.view(B, C, H * W)\n",
        "        w = torch.bmm(q, k) * (int(C) ** (-0.5))\n",
        "        assert list(w.shape) == [B, H * W, H * W]\n",
        "        w = F.softmax(w, dim=-1)\n",
        "\n",
        "        v = v.permute(0, 2, 3, 1).view(B, H * W, C)\n",
        "        h = torch.bmm(w, v)\n",
        "        assert list(h.shape) == [B, H * W, C]\n",
        "        h = h.view(B, H, W, C).permute(0, 3, 1, 2)\n",
        "        h = self.proj(h)\n",
        "\n",
        "        return x + h\n",
        "\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, tdim, dropout, attn=True):\n",
        "        super().__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.GroupNorm(32, in_ch),\n",
        "            SineAct(omega_0=1.0),\n",
        "            nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1),\n",
        "        )\n",
        "        self.temb_proj = SineLayer(\n",
        "                in_features=tdim,\n",
        "                out_features=out_ch,   # keep dim the same before final head\n",
        "                bias=True,\n",
        "                is_first=False,      # not your very first SIREN layer\n",
        "                omega_0=1.0\n",
        "            )\n",
        "            # Swish(),\n",
        "            # nn.Linear(tdim, out_ch),\n",
        "        # )\n",
        "        self.cond_proj = SineLayer(\n",
        "                in_features=tdim,\n",
        "                out_features=out_ch,   # keep dim the same before final head\n",
        "                bias=True,\n",
        "                is_first=False,      # not your very first SIREN layer\n",
        "                omega_0=1.0\n",
        "            )\n",
        "            # Swish(),\n",
        "            # nn.Linear(tdim, out_ch),\n",
        "        # )\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.GroupNorm(32, out_ch),\n",
        "            SineAct(omega_0=1.0),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1),\n",
        "        )\n",
        "        if in_ch != out_ch:\n",
        "            self.shortcut = nn.Conv2d(in_ch, out_ch, 1, stride=1, padding=0)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "        if attn:\n",
        "            self.attn = AttnBlock(out_ch)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "\n",
        "\n",
        "    def forward(self, x, temb, labels):\n",
        "        h = self.block1(x)\n",
        "        h += self.temb_proj(temb)[:, :, None, None]\n",
        "        h += self.cond_proj(labels)[:, :, None, None]\n",
        "        h = self.block2(h)\n",
        "\n",
        "        h = h + self.shortcut(x)\n",
        "        h = self.attn(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, T, num_labels, ch, ch_mult, num_res_blocks, dropout):\n",
        "        super().__init__()\n",
        "        tdim = ch * 4\n",
        "        self.time_embedding = TimeEmbedding(T, ch, tdim)\n",
        "        self.cond_embedding = ConditionalEmbedding(num_labels, ch, tdim)\n",
        "        self.head = nn.Conv2d(3, ch, kernel_size=3, stride=1, padding=1)\n",
        "        self.downblocks = nn.ModuleList()\n",
        "        chs = [ch]  # record output channel when dowmsample for upsample\n",
        "        now_ch = ch\n",
        "        for i, mult in enumerate(ch_mult):\n",
        "            out_ch = ch * mult\n",
        "            for _ in range(num_res_blocks):\n",
        "                self.downblocks.append(ResBlock(in_ch=now_ch, out_ch=out_ch, tdim=tdim, dropout=dropout))\n",
        "                now_ch = out_ch\n",
        "                chs.append(now_ch)\n",
        "            if i != len(ch_mult) - 1:\n",
        "                self.downblocks.append(DownSample(now_ch))\n",
        "                chs.append(now_ch)\n",
        "\n",
        "        self.middleblocks = nn.ModuleList([\n",
        "            ResBlock(now_ch, now_ch, tdim, dropout, attn=True),\n",
        "            ResBlock(now_ch, now_ch, tdim, dropout, attn=False),\n",
        "        ])\n",
        "\n",
        "        self.upblocks = nn.ModuleList()\n",
        "        for i, mult in reversed(list(enumerate(ch_mult))):\n",
        "            out_ch = ch * mult\n",
        "            for _ in range(num_res_blocks + 1):\n",
        "                self.upblocks.append(ResBlock(in_ch=chs.pop() + now_ch, out_ch=out_ch, tdim=tdim, dropout=dropout, attn=False))\n",
        "                now_ch = out_ch\n",
        "            if i != 0:\n",
        "                self.upblocks.append(UpSample(now_ch))\n",
        "        assert len(chs) == 0\n",
        "\n",
        "        self.tail = nn.Sequential(\n",
        "            nn.GroupNorm(32, now_ch),\n",
        "            SineAct(omega_0=1.0),\n",
        "            nn.Conv2d(now_ch, 3, 3, stride=1, padding=1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, t, labels):\n",
        "        # Timestep embedding\n",
        "        temb = self.time_embedding(t)\n",
        "        cemb = self.cond_embedding(labels)\n",
        "        # Downsampling\n",
        "        h = self.head(x)\n",
        "        hs = [h]\n",
        "        for layer in self.downblocks:\n",
        "            h = layer(h, temb, cemb)\n",
        "            hs.append(h)\n",
        "        # Middle\n",
        "        for layer in self.middleblocks:\n",
        "            h = layer(h, temb, cemb)\n",
        "        # Upsampling\n",
        "        for layer in self.upblocks:\n",
        "            if isinstance(layer, ResBlock):\n",
        "                h = torch.cat([h, hs.pop()], dim=1)\n",
        "            h = layer(h, temb, cemb)\n",
        "        h = self.tail(h)\n",
        "\n",
        "        assert len(hs) == 0\n",
        "        return h"
      ],
      "metadata": {
        "id": "iKaRoWTIyDaB",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1747043131408,
          "user_tz": 240,
          "elapsed": 288,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "iKaRoWTIyDaB",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract(v, t, x_shape):\n",
        "    \"\"\"\n",
        "    Extract some coefficients at specified timesteps, then reshape to\n",
        "    [batch_size, 1, 1, 1, 1, ...] for broadcasting purposes.\n",
        "    \"\"\"\n",
        "    device = t.device\n",
        "    out = torch.gather(v, index=t, dim=0).float().to(device)\n",
        "    return out.view([t.shape[0]] + [1] * (len(x_shape) - 1))\n",
        "\n",
        "\n",
        "class GaussianDiffusionTrainer(nn.Module):\n",
        "    def __init__(self, model, beta_1, beta_T, T):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.T = T\n",
        "\n",
        "        self.register_buffer(\n",
        "            'betas', torch.linspace(beta_1, beta_T, T).double())\n",
        "        alphas = 1. - self.betas\n",
        "        alphas_bar = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
        "        self.register_buffer(\n",
        "            'sqrt_alphas_bar', torch.sqrt(alphas_bar))\n",
        "        self.register_buffer(\n",
        "            'sqrt_one_minus_alphas_bar', torch.sqrt(1. - alphas_bar))\n",
        "\n",
        "    def forward(self, x_0, labels):\n",
        "        \"\"\"\n",
        "        Algorithm 1.\n",
        "        \"\"\"\n",
        "        t = torch.randint(self.T, size=(x_0.shape[0], ), device=x_0.device)\n",
        "        noise = torch.randn_like(x_0)\n",
        "        x_t =   extract(self.sqrt_alphas_bar, t, x_0.shape) * x_0 + \\\n",
        "                extract(self.sqrt_one_minus_alphas_bar, t, x_0.shape) * noise\n",
        "        loss = F.mse_loss(self.model(x_t, t, labels), noise, reduction='none')\n",
        "        return loss\n",
        "\n",
        "\n",
        "class GaussianDiffusionSampler(nn.Module):\n",
        "    def __init__(self, model, beta_1, beta_T, T, w = 0.):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.T = T\n",
        "        ### In the classifier free guidence paper, w is the key to control the gudience.\n",
        "        ### w = 0 and with label = 0 means no guidence.\n",
        "        ### w > 0 and label > 0 means guidence. Guidence would be stronger if w is bigger.\n",
        "        self.w = w\n",
        "\n",
        "        self.register_buffer('betas', torch.linspace(beta_1, beta_T, T).double())\n",
        "        alphas = 1. - self.betas\n",
        "        alphas_bar = torch.cumprod(alphas, dim=0)\n",
        "        alphas_bar_prev = F.pad(alphas_bar, [1, 0], value=1)[:T]\n",
        "        self.register_buffer('coeff1', torch.sqrt(1. / alphas))\n",
        "        self.register_buffer('coeff2', self.coeff1 * (1. - alphas) / torch.sqrt(1. - alphas_bar))\n",
        "        self.register_buffer('posterior_var', self.betas * (1. - alphas_bar_prev) / (1. - alphas_bar))\n",
        "\n",
        "    def predict_xt_prev_mean_from_eps(self, x_t, t, eps):\n",
        "        assert x_t.shape == eps.shape\n",
        "        return extract(self.coeff1, t, x_t.shape) * x_t - extract(self.coeff2, t, x_t.shape) * eps\n",
        "\n",
        "    def p_mean_variance(self, x_t, t, labels):\n",
        "        # below: only log_variance is used in the KL computations\n",
        "        var = torch.cat([self.posterior_var[1:2], self.betas[1:]])\n",
        "        var = extract(var, t, x_t.shape)\n",
        "        eps = self.model(x_t, t, labels)\n",
        "        nonEps = self.model(x_t, t, torch.zeros_like(labels).to(labels.device))\n",
        "        eps = (1. + self.w) * eps - self.w * nonEps\n",
        "        xt_prev_mean = self.predict_xt_prev_mean_from_eps(x_t, t, eps=eps)\n",
        "        return xt_prev_mean, var\n",
        "\n",
        "    def forward(self, x_T, labels):\n",
        "        \"\"\"\n",
        "        Algorithm 2.\n",
        "        \"\"\"\n",
        "        x_t = x_T\n",
        "        for time_step in reversed(range(self.T)):\n",
        "            print(time_step)\n",
        "            t = x_t.new_ones([x_T.shape[0], ], dtype=torch.long) * time_step\n",
        "            mean, var= self.p_mean_variance(x_t=x_t, t=t, labels=labels)\n",
        "            if time_step > 0:\n",
        "                noise = torch.randn_like(x_t)\n",
        "            else:\n",
        "                noise = 0\n",
        "            x_t = mean + torch.sqrt(var) * noise\n",
        "            assert torch.isnan(x_t).int().sum() == 0, \"nan in tensor.\"\n",
        "        x_0 = x_t\n",
        "        return torch.clip(x_0, -1, 1)"
      ],
      "metadata": {
        "id": "7gAu_TkzyIIh",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1747043137751,
          "user_tz": 240,
          "elapsed": 129,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "7gAu_TkzyIIh",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.fftpack import dct, idct\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def dct_batch(images: torch.Tensor) -> torch.Tensor:\n",
        "    images_np = images.cpu().numpy()\n",
        "    B,C,H,W = images_np.shape\n",
        "    dct_images = np.zeros_like(images_np)\n",
        "    for b in range(B):\n",
        "        for c in range(C):\n",
        "            dct_images[b,c] = dct(dct(images_np[b,c], axis=0, norm='ortho'), axis=1, norm='ortho')\n",
        "    return torch.from_numpy(dct_images).to(images.device)\n",
        "\n",
        "def idct_batch(dct_images: torch.Tensor) -> torch.Tensor:\n",
        "    # Convert to numpy and apply IDCT per channel\n",
        "    dct_np = dct_images.cpu().numpy()\n",
        "    B, C, H, W = dct_np.shape\n",
        "    recon_images = np.zeros_like(dct_np)\n",
        "\n",
        "    for b in range(B):\n",
        "        for c in range(C):\n",
        "            recon_images[b, c] = idct(idct(dct_np[b, c], axis=1, norm='ortho'), axis=0, norm='ortho')\n",
        "\n",
        "    return torch.from_numpy(recon_images).to(dct_images.device)\n",
        "##\n",
        "\n",
        "def train(modelConfig: Dict):\n",
        "    device = torch.device(modelConfig[\"device\"])\n",
        "    # dataset\n",
        "    dataset = CIFAR10(\n",
        "        root='./CIFAR10', train=True, download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ]))\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=modelConfig[\"batch_size\"], shuffle=True, num_workers=4, drop_last=True, pin_memory=True)\n",
        "\n",
        "    # model setup\n",
        "    net_model = UNet(T=modelConfig[\"T\"], num_labels=10, ch=modelConfig[\"channel\"], ch_mult=modelConfig[\"channel_mult\"],\n",
        "                     num_res_blocks=modelConfig[\"num_res_blocks\"], dropout=modelConfig[\"dropout\"]).to(device)\n",
        "    if modelConfig[\"training_load_weight\"] is not None:\n",
        "        net_model.load_state_dict(torch.load(os.path.join(\n",
        "            modelConfig[\"save_dir\"], modelConfig[\"training_load_weight\"]), map_location=device), strict=False)\n",
        "        print(\"Model weight load down.\")\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        net_model.parameters(), lr=modelConfig[\"lr\"], weight_decay=1e-4)\n",
        "    cosineScheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer=optimizer, T_max=modelConfig[\"epoch\"], eta_min=0, last_epoch=-1)\n",
        "    warmUpScheduler = GradualWarmupScheduler(optimizer=optimizer, multiplier=modelConfig[\"multiplier\"],\n",
        "                                             warm_epoch=modelConfig[\"epoch\"] // 10, after_scheduler=cosineScheduler)\n",
        "    trainer = GaussianDiffusionTrainer(\n",
        "        net_model, modelConfig[\"beta_1\"], modelConfig[\"beta_T\"], modelConfig[\"T\"]).to(device)\n",
        "\n",
        "    # start training\n",
        "    losses = []\n",
        "    lrs = []\n",
        "    for e in range(modelConfig[\"epoch\"]):\n",
        "        with tqdm(dataloader, dynamic_ncols=True) as tqdmDataLoader:\n",
        "            epoch_losses = []\n",
        "            epoch_lrs = []\n",
        "            for images_, labels in tqdmDataLoader:\n",
        "                # train\n",
        "                images = dct_batch(images_)\n",
        "                b = images.shape[0]\n",
        "                optimizer.zero_grad()\n",
        "                x_0 = images.to(device)\n",
        "                labels = labels.to(device) + 1\n",
        "                if np.random.rand() < 0.1:\n",
        "                    labels = torch.zeros_like(labels).to(device)\n",
        "                loss = trainer(x_0, labels).sum() / b ** 2.\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    net_model.parameters(), modelConfig[\"grad_clip\"])\n",
        "                optimizer.step()\n",
        "                loss_item = loss.item()\n",
        "                lr = optimizer.state_dict()['param_groups'][0][\"lr\"]\n",
        "                epoch_lrs.append(lr)\n",
        "                epoch_losses.append(loss_item)\n",
        "                tqdmDataLoader.set_postfix(ordered_dict={\n",
        "                    \"epoch\": e,\n",
        "                    \"loss: \": loss_item,\n",
        "                    \"img shape: \": x_0.shape,\n",
        "                    \"LR\": lr\n",
        "                })\n",
        "            tqdmDataLoader.set_postfix(ordered_dict={\n",
        "                \"epoch\": e,\n",
        "                \"loss: \": np.mean(epoch_losses),\n",
        "                \"img shape: \": x_0.shape,\n",
        "                \"LR\": np.mean(epoch_lrs)\n",
        "            })\n",
        "        warmUpScheduler.step()\n",
        "        losses.append(np.mean(epoch_losses))\n",
        "        lrs.append(np.mean(epoch_lrs))\n",
        "        torch.save(net_model.state_dict(), os.path.join(\n",
        "            modelConfig[\"save_dir\"], 'ckpt_' + str(e) + \"_.pt\"))\n",
        "    print(losses)\n",
        "    print(lrs)\n",
        "    with open('./data.txt', 'w') as f:\n",
        "        f.write(f\"{losses}\\n{lrs}\")\n",
        "\n",
        "\n",
        "def eval(modelConfig: Dict):\n",
        "    device = torch.device(modelConfig[\"device\"])\n",
        "    # load model and evaluate\n",
        "    with torch.no_grad():\n",
        "        step = int(modelConfig[\"batch_size\"] // 10)\n",
        "        labelList = []\n",
        "        k = 0\n",
        "        for i in range(1, modelConfig[\"batch_size\"] + 1):\n",
        "            labelList.append(torch.ones(size=[1]).long() * k)\n",
        "            if i % step == 0:\n",
        "                if k < 10 - 1:\n",
        "                    k += 1\n",
        "        labels = torch.cat(labelList, dim=0).long().to(device) + 1\n",
        "        print(\"labels: \", labels)\n",
        "        model = UNet(T=modelConfig[\"T\"], num_labels=10, ch=modelConfig[\"channel\"], ch_mult=modelConfig[\"channel_mult\"],\n",
        "                     num_res_blocks=modelConfig[\"num_res_blocks\"], dropout=modelConfig[\"dropout\"]).to(device)\n",
        "        ckpt = torch.load(os.path.join(\n",
        "            modelConfig[\"save_dir\"], modelConfig[\"test_load_weight\"]), map_location=device)\n",
        "        model.load_state_dict(ckpt)\n",
        "        print(\"model load weight done.\")\n",
        "        model.eval()\n",
        "        sampler = GaussianDiffusionSampler(\n",
        "            model, modelConfig[\"beta_1\"], modelConfig[\"beta_T\"], modelConfig[\"T\"], w=modelConfig[\"w\"]).to(device)\n",
        "        # Sampled from standard normal distribution\n",
        "        noisyImage = torch.randn(\n",
        "            size=[modelConfig[\"batch_size\"], 3, modelConfig[\"img_size\"], modelConfig[\"img_size\"]], device=device)\n",
        "        saveNoisy = torch.clamp(noisyImage * 0.5 + 0.5, 0, 1)\n",
        "        save_image(saveNoisy, os.path.join(\n",
        "            modelConfig[\"sampled_dir\"],  modelConfig[\"sampledNoisyImgName\"]), nrow=modelConfig[\"nrow\"])\n",
        "        sampledImgs = sampler(noisyImage, labels)\n",
        "        sampledImgs = sampledImgs * 0.5 + 0.5  # [0 ~ 1]\n",
        "        print(sampledImgs)\n",
        "        save_image(sampledImgs, os.path.join(\n",
        "            modelConfig[\"sampled_dir\"],  modelConfig[\"sampledImgName\"]), nrow=modelConfig[\"nrow\"])"
      ],
      "metadata": {
        "id": "_NgkLcSDyDl9",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1747068729965,
          "user_tz": 240,
          "elapsed": 157,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "_NgkLcSDyDl9",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelConfig = {\n",
        "    \"state\": \"eval\", # or eval\n",
        "    \"epoch\": 70,\n",
        "    \"batch_size\": 80,\n",
        "    \"T\": 500,\n",
        "    \"channel\": 128,\n",
        "    \"channel_mult\": [1, 2, 2, 2],\n",
        "    \"num_res_blocks\": 2,\n",
        "    \"dropout\": 0.15,\n",
        "    \"lr\": 1e-4,\n",
        "    \"multiplier\": 2.5,\n",
        "    \"beta_1\": 1e-4,\n",
        "    \"beta_T\": 0.028,\n",
        "    \"img_size\": 32,\n",
        "    \"grad_clip\": 1.,\n",
        "    \"device\": \"cuda:0\",\n",
        "    \"w\": 1.8,\n",
        "    \"save_dir\": \"./CheckpointsCondition2/\",\n",
        "    \"training_load_weight\": None,\n",
        "    \"test_load_weight\": \"ckpt_69_.pt\",\n",
        "    \"sampled_dir\": \"./SampledImgs/\",\n",
        "    \"sampledNoisyImgName\": \"NoisyGuidenceImgs.png\",\n",
        "    \"sampledImgName\": \"SampledGuidenceImgs.png\",\n",
        "    \"nrow\": 8\n",
        "}\n",
        "if modelConfig[\"state\"] == \"train\":\n",
        "    train(modelConfig)\n",
        "else:\n",
        "    eval(modelConfig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPooMmX2yDnC",
        "outputId": "44776ac2-710e-4ea9-ac99-95bd48946aaa",
        "collapsed": true,
        "executionInfo": {
          "status": "ok",
          "timestamp": 1747062331356,
          "user_tz": 240,
          "elapsed": 104283,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "iPooMmX2yDnC",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels:  tensor([ 1,  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,\n",
            "         3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,\n",
            "         5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  7,  7,  7,  7,  7,  7,\n",
            "         7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,\n",
            "        10, 10, 10, 10, 10, 10, 10, 10], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-81e022935095>:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(os.path.join(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model load weight done.\n",
            "499\n",
            "498\n",
            "497\n",
            "496\n",
            "495\n",
            "494\n",
            "493\n",
            "492\n",
            "491\n",
            "490\n",
            "489\n",
            "488\n",
            "487\n",
            "486\n",
            "485\n",
            "484\n",
            "483\n",
            "482\n",
            "481\n",
            "480\n",
            "479\n",
            "478\n",
            "477\n",
            "476\n",
            "475\n",
            "474\n",
            "473\n",
            "472\n",
            "471\n",
            "470\n",
            "469\n",
            "468\n",
            "467\n",
            "466\n",
            "465\n",
            "464\n",
            "463\n",
            "462\n",
            "461\n",
            "460\n",
            "459\n",
            "458\n",
            "457\n",
            "456\n",
            "455\n",
            "454\n",
            "453\n",
            "452\n",
            "451\n",
            "450\n",
            "449\n",
            "448\n",
            "447\n",
            "446\n",
            "445\n",
            "444\n",
            "443\n",
            "442\n",
            "441\n",
            "440\n",
            "439\n",
            "438\n",
            "437\n",
            "436\n",
            "435\n",
            "434\n",
            "433\n",
            "432\n",
            "431\n",
            "430\n",
            "429\n",
            "428\n",
            "427\n",
            "426\n",
            "425\n",
            "424\n",
            "423\n",
            "422\n",
            "421\n",
            "420\n",
            "419\n",
            "418\n",
            "417\n",
            "416\n",
            "415\n",
            "414\n",
            "413\n",
            "412\n",
            "411\n",
            "410\n",
            "409\n",
            "408\n",
            "407\n",
            "406\n",
            "405\n",
            "404\n",
            "403\n",
            "402\n",
            "401\n",
            "400\n",
            "399\n",
            "398\n",
            "397\n",
            "396\n",
            "395\n",
            "394\n",
            "393\n",
            "392\n",
            "391\n",
            "390\n",
            "389\n",
            "388\n",
            "387\n",
            "386\n",
            "385\n",
            "384\n",
            "383\n",
            "382\n",
            "381\n",
            "380\n",
            "379\n",
            "378\n",
            "377\n",
            "376\n",
            "375\n",
            "374\n",
            "373\n",
            "372\n",
            "371\n",
            "370\n",
            "369\n",
            "368\n",
            "367\n",
            "366\n",
            "365\n",
            "364\n",
            "363\n",
            "362\n",
            "361\n",
            "360\n",
            "359\n",
            "358\n",
            "357\n",
            "356\n",
            "355\n",
            "354\n",
            "353\n",
            "352\n",
            "351\n",
            "350\n",
            "349\n",
            "348\n",
            "347\n",
            "346\n",
            "345\n",
            "344\n",
            "343\n",
            "342\n",
            "341\n",
            "340\n",
            "339\n",
            "338\n",
            "337\n",
            "336\n",
            "335\n",
            "334\n",
            "333\n",
            "332\n",
            "331\n",
            "330\n",
            "329\n",
            "328\n",
            "327\n",
            "326\n",
            "325\n",
            "324\n",
            "323\n",
            "322\n",
            "321\n",
            "320\n",
            "319\n",
            "318\n",
            "317\n",
            "316\n",
            "315\n",
            "314\n",
            "313\n",
            "312\n",
            "311\n",
            "310\n",
            "309\n",
            "308\n",
            "307\n",
            "306\n",
            "305\n",
            "304\n",
            "303\n",
            "302\n",
            "301\n",
            "300\n",
            "299\n",
            "298\n",
            "297\n",
            "296\n",
            "295\n",
            "294\n",
            "293\n",
            "292\n",
            "291\n",
            "290\n",
            "289\n",
            "288\n",
            "287\n",
            "286\n",
            "285\n",
            "284\n",
            "283\n",
            "282\n",
            "281\n",
            "280\n",
            "279\n",
            "278\n",
            "277\n",
            "276\n",
            "275\n",
            "274\n",
            "273\n",
            "272\n",
            "271\n",
            "270\n",
            "269\n",
            "268\n",
            "267\n",
            "266\n",
            "265\n",
            "264\n",
            "263\n",
            "262\n",
            "261\n",
            "260\n",
            "259\n",
            "258\n",
            "257\n",
            "256\n",
            "255\n",
            "254\n",
            "253\n",
            "252\n",
            "251\n",
            "250\n",
            "249\n",
            "248\n",
            "247\n",
            "246\n",
            "245\n",
            "244\n",
            "243\n",
            "242\n",
            "241\n",
            "240\n",
            "239\n",
            "238\n",
            "237\n",
            "236\n",
            "235\n",
            "234\n",
            "233\n",
            "232\n",
            "231\n",
            "230\n",
            "229\n",
            "228\n",
            "227\n",
            "226\n",
            "225\n",
            "224\n",
            "223\n",
            "222\n",
            "221\n",
            "220\n",
            "219\n",
            "218\n",
            "217\n",
            "216\n",
            "215\n",
            "214\n",
            "213\n",
            "212\n",
            "211\n",
            "210\n",
            "209\n",
            "208\n",
            "207\n",
            "206\n",
            "205\n",
            "204\n",
            "203\n",
            "202\n",
            "201\n",
            "200\n",
            "199\n",
            "198\n",
            "197\n",
            "196\n",
            "195\n",
            "194\n",
            "193\n",
            "192\n",
            "191\n",
            "190\n",
            "189\n",
            "188\n",
            "187\n",
            "186\n",
            "185\n",
            "184\n",
            "183\n",
            "182\n",
            "181\n",
            "180\n",
            "179\n",
            "178\n",
            "177\n",
            "176\n",
            "175\n",
            "174\n",
            "173\n",
            "172\n",
            "171\n",
            "170\n",
            "169\n",
            "168\n",
            "167\n",
            "166\n",
            "165\n",
            "164\n",
            "163\n",
            "162\n",
            "161\n",
            "160\n",
            "159\n",
            "158\n",
            "157\n",
            "156\n",
            "155\n",
            "154\n",
            "153\n",
            "152\n",
            "151\n",
            "150\n",
            "149\n",
            "148\n",
            "147\n",
            "146\n",
            "145\n",
            "144\n",
            "143\n",
            "142\n",
            "141\n",
            "140\n",
            "139\n",
            "138\n",
            "137\n",
            "136\n",
            "135\n",
            "134\n",
            "133\n",
            "132\n",
            "131\n",
            "130\n",
            "129\n",
            "128\n",
            "127\n",
            "126\n",
            "125\n",
            "124\n",
            "123\n",
            "122\n",
            "121\n",
            "120\n",
            "119\n",
            "118\n",
            "117\n",
            "116\n",
            "115\n",
            "114\n",
            "113\n",
            "112\n",
            "111\n",
            "110\n",
            "109\n",
            "108\n",
            "107\n",
            "106\n",
            "105\n",
            "104\n",
            "103\n",
            "102\n",
            "101\n",
            "100\n",
            "99\n",
            "98\n",
            "97\n",
            "96\n",
            "95\n",
            "94\n",
            "93\n",
            "92\n",
            "91\n",
            "90\n",
            "89\n",
            "88\n",
            "87\n",
            "86\n",
            "85\n",
            "84\n",
            "83\n",
            "82\n",
            "81\n",
            "80\n",
            "79\n",
            "78\n",
            "77\n",
            "76\n",
            "75\n",
            "74\n",
            "73\n",
            "72\n",
            "71\n",
            "70\n",
            "69\n",
            "68\n",
            "67\n",
            "66\n",
            "65\n",
            "64\n",
            "63\n",
            "62\n",
            "61\n",
            "60\n",
            "59\n",
            "58\n",
            "57\n",
            "56\n",
            "55\n",
            "54\n",
            "53\n",
            "52\n",
            "51\n",
            "50\n",
            "49\n",
            "48\n",
            "47\n",
            "46\n",
            "45\n",
            "44\n",
            "43\n",
            "42\n",
            "41\n",
            "40\n",
            "39\n",
            "38\n",
            "37\n",
            "36\n",
            "35\n",
            "34\n",
            "33\n",
            "32\n",
            "31\n",
            "30\n",
            "29\n",
            "28\n",
            "27\n",
            "26\n",
            "25\n",
            "24\n",
            "23\n",
            "22\n",
            "21\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "0\n",
            "tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.5336, 0.4995, 0.4621],\n",
            "          [0.2072, 0.0241, 0.5858,  ..., 0.5160, 0.5141, 0.5046],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.4916, 0.5265, 0.5363],\n",
            "          ...,\n",
            "          [0.4250, 0.5062, 0.5024,  ..., 0.5019, 0.4900, 0.5041],\n",
            "          [0.4799, 0.4843, 0.4977,  ..., 0.4975, 0.5081, 0.5089],\n",
            "          [0.4779, 0.4676, 0.5076,  ..., 0.5004, 0.5199, 0.4908]],\n",
            "\n",
            "         [[1.0000, 0.2383, 1.0000,  ..., 0.5081, 0.5010, 0.5079],\n",
            "          [1.0000, 0.2525, 0.5970,  ..., 0.5054, 0.4831, 0.5121],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.4930, 0.5062, 0.5223],\n",
            "          ...,\n",
            "          [0.4731, 0.4859, 0.4920,  ..., 0.4841, 0.5001, 0.4999],\n",
            "          [0.4984, 0.4885, 0.4806,  ..., 0.5281, 0.5263, 0.5012],\n",
            "          [0.4685, 0.4719, 0.4998,  ..., 0.4997, 0.5065, 0.5019]],\n",
            "\n",
            "         [[1.0000, 0.8773, 1.0000,  ..., 0.5331, 0.4873, 0.4735],\n",
            "          [1.0000, 0.3426, 0.7981,  ..., 0.5080, 0.5024, 0.5074],\n",
            "          [0.0000, 0.0000, 0.0000,  ..., 0.5068, 0.5092, 0.5060],\n",
            "          ...,\n",
            "          [0.4786, 0.5031, 0.5236,  ..., 0.5188, 0.5131, 0.5005],\n",
            "          [0.4958, 0.5155, 0.4822,  ..., 0.4859, 0.4938, 0.5068],\n",
            "          [0.5164, 0.4801, 0.5121,  ..., 0.4933, 0.4983, 0.5168]]],\n",
            "\n",
            "\n",
            "        [[[0.0000, 1.0000, 1.0000,  ..., 0.4761, 0.5316, 0.5474],\n",
            "          [1.0000, 0.0000, 0.0826,  ..., 0.5163, 0.4883, 0.4347],\n",
            "          [1.0000, 0.0000, 0.0000,  ..., 0.5265, 0.4919, 0.5110],\n",
            "          ...,\n",
            "          [0.4983, 0.5251, 0.5269,  ..., 0.4969, 0.5181, 0.5240],\n",
            "          [0.4794, 0.5094, 0.4557,  ..., 0.5125, 0.4940, 0.4910],\n",
            "          [0.4953, 0.4766, 0.5411,  ..., 0.5101, 0.5336, 0.5163]],\n",
            "\n",
            "         [[0.0000, 1.0000, 1.0000,  ..., 0.4917, 0.5289, 0.4761],\n",
            "          [1.0000, 0.0000, 0.0000,  ..., 0.4658, 0.4879, 0.5184],\n",
            "          [1.0000, 0.0000, 0.0000,  ..., 0.4899, 0.4866, 0.4693],\n",
            "          ...,\n",
            "          [0.5397, 0.5042, 0.5262,  ..., 0.4936, 0.5571, 0.4666],\n",
            "          [0.4385, 0.5185, 0.4746,  ..., 0.5077, 0.5105, 0.5192],\n",
            "          [0.5126, 0.4838, 0.5238,  ..., 0.4633, 0.4847, 0.4884]],\n",
            "\n",
            "         [[1.0000, 1.0000, 1.0000,  ..., 0.4672, 0.4604, 0.5513],\n",
            "          [1.0000, 0.0000, 0.3463,  ..., 0.5043, 0.4841, 0.5244],\n",
            "          [1.0000, 0.0000, 0.0000,  ..., 0.4883, 0.4791, 0.5036],\n",
            "          ...,\n",
            "          [0.5183, 0.5079, 0.4627,  ..., 0.5144, 0.5329, 0.5212],\n",
            "          [0.4739, 0.5077, 0.4986,  ..., 0.5588, 0.4916, 0.4723],\n",
            "          [0.5029, 0.4864, 0.5111,  ..., 0.4744, 0.5062, 0.5207]]],\n",
            "\n",
            "\n",
            "        [[[0.0000, 0.0000, 1.0000,  ..., 0.5940, 0.5569, 0.5368],\n",
            "          [1.0000, 0.4838, 0.8844,  ..., 0.4851, 0.4869, 0.4660],\n",
            "          [1.0000, 0.0000, 0.0000,  ..., 0.4683, 0.5397, 0.4990],\n",
            "          ...,\n",
            "          [0.4746, 0.5522, 0.4788,  ..., 0.4690, 0.5092, 0.5511],\n",
            "          [0.4737, 0.4782, 0.5312,  ..., 0.4733, 0.4614, 0.5263],\n",
            "          [0.5766, 0.5050, 0.4950,  ..., 0.4806, 0.5039, 0.4857]],\n",
            "\n",
            "         [[1.0000, 0.0000, 1.0000,  ..., 0.5410, 0.5206, 0.5654],\n",
            "          [1.0000, 0.8791, 1.0000,  ..., 0.4489, 0.4791, 0.4506],\n",
            "          [1.0000, 0.2656, 0.0000,  ..., 0.5117, 0.4520, 0.5325],\n",
            "          ...,\n",
            "          [0.4666, 0.5134, 0.4847,  ..., 0.5032, 0.4529, 0.4772],\n",
            "          [0.3683, 0.4778, 0.4793,  ..., 0.4765, 0.5162, 0.5165],\n",
            "          [0.4804, 0.5023, 0.4885,  ..., 0.5343, 0.4323, 0.5046]],\n",
            "\n",
            "         [[1.0000, 0.0000, 1.0000,  ..., 0.5649, 0.5095, 0.3827],\n",
            "          [1.0000, 0.5722, 1.0000,  ..., 0.5018, 0.4457, 0.5488],\n",
            "          [1.0000, 1.0000, 0.0000,  ..., 0.4948, 0.4699, 0.5511],\n",
            "          ...,\n",
            "          [0.4545, 0.4870, 0.4553,  ..., 0.4855, 0.6269, 0.5530],\n",
            "          [0.4772, 0.5007, 0.5128,  ..., 0.5173, 0.5219, 0.4741],\n",
            "          [0.4548, 0.4556, 0.4795,  ..., 0.5776, 0.4773, 0.5118]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.0000, 0.0000, 1.0000,  ..., 0.5397, 0.5371, 0.5162],\n",
            "          [1.0000, 0.0000, 0.2649,  ..., 0.4941, 0.4701, 0.4905],\n",
            "          [1.0000, 0.0000, 0.9465,  ..., 0.4890, 0.4934, 0.5032],\n",
            "          ...,\n",
            "          [0.5003, 0.5196, 0.5093,  ..., 0.4969, 0.5024, 0.4908],\n",
            "          [0.5042, 0.4877, 0.5049,  ..., 0.4956, 0.4923, 0.4907],\n",
            "          [0.4760, 0.4715, 0.5191,  ..., 0.5050, 0.4924, 0.5317]],\n",
            "\n",
            "         [[0.0000, 0.0000, 1.0000,  ..., 0.5300, 0.5289, 0.5108],\n",
            "          [1.0000, 0.0000, 0.3523,  ..., 0.5049, 0.4801, 0.4927],\n",
            "          [1.0000, 0.0000, 0.9056,  ..., 0.4899, 0.4956, 0.5065],\n",
            "          ...,\n",
            "          [0.5162, 0.5122, 0.5083,  ..., 0.5027, 0.5018, 0.4975],\n",
            "          [0.5011, 0.4909, 0.5089,  ..., 0.4970, 0.4896, 0.4983],\n",
            "          [0.4829, 0.4720, 0.5192,  ..., 0.5196, 0.4963, 0.5189]],\n",
            "\n",
            "         [[0.0000, 0.0000, 1.0000,  ..., 0.5340, 0.5246, 0.5078],\n",
            "          [1.0000, 0.0000, 0.0351,  ..., 0.4953, 0.4756, 0.4921],\n",
            "          [1.0000, 0.0000, 1.0000,  ..., 0.4889, 0.4839, 0.5053],\n",
            "          ...,\n",
            "          [0.4912, 0.5183, 0.5153,  ..., 0.5068, 0.5046, 0.4942],\n",
            "          [0.5011, 0.4879, 0.5002,  ..., 0.4985, 0.5022, 0.4989],\n",
            "          [0.4778, 0.4614, 0.5186,  ..., 0.5091, 0.5014, 0.5302]]],\n",
            "\n",
            "\n",
            "        [[[0.0000, 1.0000, 0.0000,  ..., 0.4791, 0.4629, 0.4528],\n",
            "          [1.0000, 1.0000, 0.0000,  ..., 0.5271, 0.5342, 0.5279],\n",
            "          [1.0000, 1.0000, 0.0000,  ..., 0.5110, 0.5427, 0.5152],\n",
            "          ...,\n",
            "          [0.4979, 0.5069, 0.4831,  ..., 0.5127, 0.5081, 0.4777],\n",
            "          [0.4719, 0.5117, 0.4709,  ..., 0.5272, 0.4986, 0.5044],\n",
            "          [0.5322, 0.4905, 0.5290,  ..., 0.5391, 0.4959, 0.5121]],\n",
            "\n",
            "         [[0.0000, 1.0000, 0.1035,  ..., 0.4811, 0.4525, 0.4547],\n",
            "          [1.0000, 1.0000, 0.0000,  ..., 0.5103, 0.5242, 0.5122],\n",
            "          [1.0000, 1.0000, 0.0000,  ..., 0.5134, 0.5331, 0.5194],\n",
            "          ...,\n",
            "          [0.5245, 0.5103, 0.4691,  ..., 0.4989, 0.5157, 0.5045],\n",
            "          [0.4871, 0.5195, 0.4892,  ..., 0.5029, 0.5039, 0.5149],\n",
            "          [0.5081, 0.5129, 0.5398,  ..., 0.5110, 0.4857, 0.4898]],\n",
            "\n",
            "         [[0.0000, 1.0000, 0.0000,  ..., 0.4729, 0.4460, 0.4703],\n",
            "          [1.0000, 1.0000, 0.0000,  ..., 0.5152, 0.5158, 0.5242],\n",
            "          [1.0000, 1.0000, 0.0000,  ..., 0.5297, 0.5379, 0.5073],\n",
            "          ...,\n",
            "          [0.5007, 0.5206, 0.4984,  ..., 0.5075, 0.5139, 0.5027],\n",
            "          [0.4938, 0.5218, 0.4753,  ..., 0.5019, 0.5046, 0.5244],\n",
            "          [0.4972, 0.5028, 0.5150,  ..., 0.5042, 0.4971, 0.5119]]],\n",
            "\n",
            "\n",
            "        [[[0.0000, 0.0000, 1.0000,  ..., 0.5250, 0.5064, 0.5140],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 0.6209, 0.4256, 0.5217],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 0.5384, 0.4101, 0.5587],\n",
            "          ...,\n",
            "          [0.5025, 0.5553, 0.4768,  ..., 0.5000, 0.5008, 0.4911],\n",
            "          [0.4961, 0.4615, 0.5015,  ..., 0.5012, 0.5140, 0.5090],\n",
            "          [0.4978, 0.4698, 0.5164,  ..., 0.5015, 0.5129, 0.5081]],\n",
            "\n",
            "         [[0.0000, 0.0000, 1.0000,  ..., 0.5719, 0.4945, 0.5301],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 0.6124, 0.4408, 0.5070],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 0.5441, 0.4049, 0.5383],\n",
            "          ...,\n",
            "          [0.5046, 0.5624, 0.4748,  ..., 0.5040, 0.5017, 0.4901],\n",
            "          [0.4855, 0.4438, 0.4961,  ..., 0.4897, 0.4983, 0.4985],\n",
            "          [0.4867, 0.4725, 0.5186,  ..., 0.5091, 0.5080, 0.5144]],\n",
            "\n",
            "         [[0.0000, 0.0000, 1.0000,  ..., 0.5818, 0.5155, 0.5426],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 0.6149, 0.4384, 0.5052],\n",
            "          [1.0000, 1.0000, 1.0000,  ..., 0.5256, 0.4179, 0.5424],\n",
            "          ...,\n",
            "          [0.4987, 0.5427, 0.4770,  ..., 0.4833, 0.4892, 0.5053],\n",
            "          [0.5133, 0.4467, 0.5026,  ..., 0.4927, 0.5182, 0.4955],\n",
            "          [0.4873, 0.4854, 0.5223,  ..., 0.5101, 0.5142, 0.5017]]]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install lpips"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9hunLR8hDonk",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1747062807993,
          "user_tz": 240,
          "elapsed": 12784,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "cb138078-5e0a-406f-d8de-f6b04d38c79b"
      },
      "id": "9hunLR8hDonk",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lpips\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from lpips) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=0.4.0->lpips) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.2.1->lpips) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->lpips) (3.0.2)\n",
            "Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lpips\n",
            "Successfully installed lpips-0.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Dict\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from scipy import linalg, stats\n",
        "import lpips\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision.models import inception_v3, Inception_V3_Weights\n",
        "from torch.nn import functional as F\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 1. METRIC FUNCTIONS\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load models\n",
        "weights = Inception_V3_Weights.DEFAULT\n",
        "_inception = inception_v3(weights=weights, aux_logits=True).to(device).eval()\n",
        "# and grab its recommended preprocessing transform:\n",
        "_incep_preprocess = weights.transforms()\n",
        "\n",
        "_lpips = lpips.LPIPS(net='alex').to(device)\n",
        "\n",
        "def _get_inception_logits(x: torch.Tensor) -> torch.Tensor:\n",
        "    x_proc = _incep_preprocess(x)           # resize → crop → normalize\n",
        "    with torch.no_grad():\n",
        "      out = _inception(x_proc.to(device))\n",
        "    if isinstance(out, tuple):\n",
        "        out = out[0]                        # drop the aux_logits\n",
        "    return out\n",
        "\n",
        "def calculate_fid(mu1, sigma1, mu2, sigma2):\n",
        "    diff = mu1 - mu2\n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    covmean = covmean.real if np.iscomplexobj(covmean) else covmean\n",
        "    return diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
        "\n",
        "def compute_fid(real: torch.Tensor, fake: torch.Tensor) -> float:\n",
        "    act_real = _get_inception_logits(real).cpu().numpy()\n",
        "    act_fake = _get_inception_logits(fake).cpu().numpy()\n",
        "    mu_r, sigma_r = act_real.mean(0), np.cov(act_real, rowvar=False)\n",
        "    mu_f, sigma_f = act_fake.mean(0), np.cov(act_fake, rowvar=False)\n",
        "    return calculate_fid(mu_r, sigma_r, mu_f, sigma_f)\n",
        "\n",
        "def compute_inception_score(fake: torch.Tensor, splits=10):\n",
        "    preds = F.softmax(_get_inception_logits(fake), dim=1).cpu().numpy()\n",
        "    N = preds.shape[0]\n",
        "    scores = []\n",
        "    for k in range(splits):\n",
        "        part = preds[k*(N//splits):(k+1)*(N//splits)]\n",
        "        py = part.mean(axis=0)\n",
        "        scores.append(np.exp(np.mean([stats.entropy(p, py) for p in part])))\n",
        "    return float(np.mean(scores)), float(np.std(scores))\n",
        "\n",
        "def compute_lpips(real: torch.Tensor, fake: torch.Tensor) -> float:\n",
        "    # LPIPS expects input in [-1,+1]\n",
        "    r = 2*real - 1\n",
        "    f = 2*fake - 1\n",
        "    with torch.no_grad():\n",
        "        return float(_lpips(f.to(device), r.to(device)).mean())\n",
        "\n",
        "def evaluate_all(real: torch.Tensor, fake: torch.Tensor) -> Dict[str, float]:\n",
        "    return {\n",
        "        \"FID\": compute_fid(real, fake),\n",
        "        \"IS_mean\": compute_inception_score(fake)[0],\n",
        "        \"IS_std\":  compute_inception_score(fake)[1],\n",
        "        \"LPIPS\": compute_lpips(real, fake),\n",
        "    }\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 2. REAL CIFAR-10 LOADER\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def load_cifar10_test(num_samples: int):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      real_images: Tensor [num_samples,3,32,32] in [0,1]\n",
        "      real_labels: Tensor [num_samples]\n",
        "    \"\"\"\n",
        "    ds = CIFAR10(root=\"./data\", train=False, download=True,\n",
        "                 transform=transforms.ToTensor())\n",
        "    if num_samples < len(ds):\n",
        "        ds = Subset(ds, list(range(num_samples)))\n",
        "    loader = DataLoader(ds, batch_size=num_samples, shuffle=False)\n",
        "    _imgs, labs = next(iter(loader))\n",
        "    imgs = dct_batch(_imgs)  # compute DCT of images before\n",
        "    return imgs.to(device), labs.to(device)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "# 3. EXTENDED eval() WITH METRICS\n",
        "# ──────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "def eval_with_metrics(modelConfig: Dict):\n",
        "    device = torch.device(modelConfig[\"device\"])\n",
        "    B = modelConfig[\"batch_size\"]\n",
        "\n",
        "    # --- load real CIFAR-10 test images\n",
        "    real_images, real_labels = load_cifar10_test(num_samples=B)\n",
        "    print(f\"Loaded {real_images.shape[0]} real CIFAR-10 test images.\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # build label tensor [0..9] repeated\n",
        "        step = B // 10\n",
        "        label_list = []\n",
        "        for k in range(10):\n",
        "            label_list += [torch.full((step,), k, dtype=torch.long)]\n",
        "        labels = torch.cat(label_list, dim=0).to(device)\n",
        "        print(\"Labels tensor:\", labels.shape)\n",
        "\n",
        "        # instantiate and load weights\n",
        "        model = UNet(\n",
        "            T=modelConfig[\"T\"],\n",
        "            num_labels=10,\n",
        "            ch=modelConfig[\"channel\"],\n",
        "            ch_mult=modelConfig[\"channel_mult\"],\n",
        "            num_res_blocks=modelConfig[\"num_res_blocks\"],\n",
        "            dropout=modelConfig[\"dropout\"]\n",
        "        ).to(device)\n",
        "        ckpt = torch.load(\n",
        "            os.path.join(modelConfig[\"save_dir\"], modelConfig[\"test_load_weight\"]),\n",
        "            map_location=device\n",
        "        )\n",
        "        model.load_state_dict(ckpt)\n",
        "        model.eval()\n",
        "        print(\"Model loaded.\")\n",
        "\n",
        "        # sampler\n",
        "        sampler = GaussianDiffusionSampler(\n",
        "            model,\n",
        "            modelConfig[\"beta_1\"],\n",
        "            modelConfig[\"beta_T\"],\n",
        "            modelConfig[\"T\"],\n",
        "            w=modelConfig[\"w\"]\n",
        "        ).to(device)\n",
        "\n",
        "        # sample from N(0,I)\n",
        "        noisy = torch.randn(\n",
        "            B, 3, modelConfig[\"img_size\"], modelConfig[\"img_size\"],\n",
        "            device=device\n",
        "        )\n",
        "        sampled = sampler(noisy, labels)\n",
        "        sampled = sampled.clamp(-0.5, +0.5) + 0.5   # to [0,1]\n",
        "        print(f\"Generated {sampled.shape[0]} fake images.\")\n",
        "\n",
        "        # (optional) save a grid\n",
        "\n",
        "        # (optional) save a grid\n",
        "        save_image(\n",
        "            sampled,\n",
        "            os.path.join(modelConfig[\"sampled_dir\"], \"SampledGuidanceImgs_both_dct.png\"),\n",
        "            nrow=modelConfig[\"nrow\"]\n",
        "        )\n",
        "        back_to_normal = idct_batch(sampled)\n",
        "        save_image(\n",
        "            back_to_normal,\n",
        "            os.path.join(modelConfig[\"sampled_dir\"], \"SampledGuidanceImgs_both_nodct.png\"),\n",
        "            nrow=modelConfig[\"nrow\"]\n",
        "        )\n",
        "        norm_to_normal = back_to_normal * 0.5 + 0.5\n",
        "        save_image(\n",
        "            norm_to_normal,\n",
        "            os.path.join(modelConfig[\"sampled_dir\"], \"SampledGuidanceImgs_both_nodct_norm.png\"),\n",
        "            nrow=modelConfig[\"nrow\"]\n",
        "        )\n",
        "        backnforth = idct_batch(dct_batch(real_images))* 0.5 + 0.5\n",
        "        save_image(\n",
        "            backnforth,\n",
        "            os.path.join(modelConfig[\"sampled_dir\"], \"SampledGuidanceImgs_both_DCTandinv.png\"),\n",
        "            nrow=modelConfig[\"nrow\"]\n",
        "        )\n",
        "\n",
        "    # --- compute metrics\n",
        "    metrics = evaluate_all(real_images, sampled)\n",
        "    print(\"=== Evaluation Metrics ===\")\n",
        "    for k,v in metrics.items():\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def main(model_config=None):\n",
        "    modelConfig = {\n",
        "        \"state\": \"eval\", # or eval\n",
        "        \"epoch\": 70,\n",
        "        \"batch_size\": 80,\n",
        "        \"T\": 500,\n",
        "        \"channel\": 128,\n",
        "        \"channel_mult\": [1, 2, 2, 2],\n",
        "        \"num_res_blocks\": 2,\n",
        "        \"dropout\": 0.15,\n",
        "        \"lr\": 1e-4,\n",
        "        \"multiplier\": 2.5,\n",
        "        \"beta_1\": 1e-4,\n",
        "        \"beta_T\": 0.028,\n",
        "        \"img_size\": 32,\n",
        "        \"grad_clip\": 1.,\n",
        "        \"device\": \"cuda:0\",\n",
        "        \"w\": 1.8,\n",
        "        \"save_dir\": \"./CheckpointsCondition2/\",\n",
        "        \"training_load_weight\": None,\n",
        "        \"test_load_weight\": \"ckpt_69_.pt\",\n",
        "        \"sampled_dir\": \"./SampledImgs/\",\n",
        "        \"sampledNoisyImgName\": \"NoisyGuidenceImgs.png\",\n",
        "        \"sampledImgName\": \"SampledGuidenceImgs.png\",\n",
        "        \"nrow\": 8\n",
        "    }\n",
        "    if model_config is not None:\n",
        "        modelConfig = model_config\n",
        "    if modelConfig[\"state\"] == \"train\":\n",
        "        training_loss = train(modelConfig)  # new\n",
        "        print(training_loss)  # new\n",
        "    else:\n",
        "        metrics = eval_with_metrics(modelConfig)\n",
        "        print(metrics)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ww1491YKDqod",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1747069166300,
          "user_tz": 240,
          "elapsed": 108046,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "7f551487-54ec-48c7-eac1-ff1a9c655d9f"
      },
      "id": "ww1491YKDqod",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Files already downloaded and verified\n",
            "Loaded 80 real CIFAR-10 test images.\n",
            "Labels tensor: torch.Size([80])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-bf9ca84b1c1e>:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "499\n",
            "498\n",
            "497\n",
            "496\n",
            "495\n",
            "494\n",
            "493\n",
            "492\n",
            "491\n",
            "490\n",
            "489\n",
            "488\n",
            "487\n",
            "486\n",
            "485\n",
            "484\n",
            "483\n",
            "482\n",
            "481\n",
            "480\n",
            "479\n",
            "478\n",
            "477\n",
            "476\n",
            "475\n",
            "474\n",
            "473\n",
            "472\n",
            "471\n",
            "470\n",
            "469\n",
            "468\n",
            "467\n",
            "466\n",
            "465\n",
            "464\n",
            "463\n",
            "462\n",
            "461\n",
            "460\n",
            "459\n",
            "458\n",
            "457\n",
            "456\n",
            "455\n",
            "454\n",
            "453\n",
            "452\n",
            "451\n",
            "450\n",
            "449\n",
            "448\n",
            "447\n",
            "446\n",
            "445\n",
            "444\n",
            "443\n",
            "442\n",
            "441\n",
            "440\n",
            "439\n",
            "438\n",
            "437\n",
            "436\n",
            "435\n",
            "434\n",
            "433\n",
            "432\n",
            "431\n",
            "430\n",
            "429\n",
            "428\n",
            "427\n",
            "426\n",
            "425\n",
            "424\n",
            "423\n",
            "422\n",
            "421\n",
            "420\n",
            "419\n",
            "418\n",
            "417\n",
            "416\n",
            "415\n",
            "414\n",
            "413\n",
            "412\n",
            "411\n",
            "410\n",
            "409\n",
            "408\n",
            "407\n",
            "406\n",
            "405\n",
            "404\n",
            "403\n",
            "402\n",
            "401\n",
            "400\n",
            "399\n",
            "398\n",
            "397\n",
            "396\n",
            "395\n",
            "394\n",
            "393\n",
            "392\n",
            "391\n",
            "390\n",
            "389\n",
            "388\n",
            "387\n",
            "386\n",
            "385\n",
            "384\n",
            "383\n",
            "382\n",
            "381\n",
            "380\n",
            "379\n",
            "378\n",
            "377\n",
            "376\n",
            "375\n",
            "374\n",
            "373\n",
            "372\n",
            "371\n",
            "370\n",
            "369\n",
            "368\n",
            "367\n",
            "366\n",
            "365\n",
            "364\n",
            "363\n",
            "362\n",
            "361\n",
            "360\n",
            "359\n",
            "358\n",
            "357\n",
            "356\n",
            "355\n",
            "354\n",
            "353\n",
            "352\n",
            "351\n",
            "350\n",
            "349\n",
            "348\n",
            "347\n",
            "346\n",
            "345\n",
            "344\n",
            "343\n",
            "342\n",
            "341\n",
            "340\n",
            "339\n",
            "338\n",
            "337\n",
            "336\n",
            "335\n",
            "334\n",
            "333\n",
            "332\n",
            "331\n",
            "330\n",
            "329\n",
            "328\n",
            "327\n",
            "326\n",
            "325\n",
            "324\n",
            "323\n",
            "322\n",
            "321\n",
            "320\n",
            "319\n",
            "318\n",
            "317\n",
            "316\n",
            "315\n",
            "314\n",
            "313\n",
            "312\n",
            "311\n",
            "310\n",
            "309\n",
            "308\n",
            "307\n",
            "306\n",
            "305\n",
            "304\n",
            "303\n",
            "302\n",
            "301\n",
            "300\n",
            "299\n",
            "298\n",
            "297\n",
            "296\n",
            "295\n",
            "294\n",
            "293\n",
            "292\n",
            "291\n",
            "290\n",
            "289\n",
            "288\n",
            "287\n",
            "286\n",
            "285\n",
            "284\n",
            "283\n",
            "282\n",
            "281\n",
            "280\n",
            "279\n",
            "278\n",
            "277\n",
            "276\n",
            "275\n",
            "274\n",
            "273\n",
            "272\n",
            "271\n",
            "270\n",
            "269\n",
            "268\n",
            "267\n",
            "266\n",
            "265\n",
            "264\n",
            "263\n",
            "262\n",
            "261\n",
            "260\n",
            "259\n",
            "258\n",
            "257\n",
            "256\n",
            "255\n",
            "254\n",
            "253\n",
            "252\n",
            "251\n",
            "250\n",
            "249\n",
            "248\n",
            "247\n",
            "246\n",
            "245\n",
            "244\n",
            "243\n",
            "242\n",
            "241\n",
            "240\n",
            "239\n",
            "238\n",
            "237\n",
            "236\n",
            "235\n",
            "234\n",
            "233\n",
            "232\n",
            "231\n",
            "230\n",
            "229\n",
            "228\n",
            "227\n",
            "226\n",
            "225\n",
            "224\n",
            "223\n",
            "222\n",
            "221\n",
            "220\n",
            "219\n",
            "218\n",
            "217\n",
            "216\n",
            "215\n",
            "214\n",
            "213\n",
            "212\n",
            "211\n",
            "210\n",
            "209\n",
            "208\n",
            "207\n",
            "206\n",
            "205\n",
            "204\n",
            "203\n",
            "202\n",
            "201\n",
            "200\n",
            "199\n",
            "198\n",
            "197\n",
            "196\n",
            "195\n",
            "194\n",
            "193\n",
            "192\n",
            "191\n",
            "190\n",
            "189\n",
            "188\n",
            "187\n",
            "186\n",
            "185\n",
            "184\n",
            "183\n",
            "182\n",
            "181\n",
            "180\n",
            "179\n",
            "178\n",
            "177\n",
            "176\n",
            "175\n",
            "174\n",
            "173\n",
            "172\n",
            "171\n",
            "170\n",
            "169\n",
            "168\n",
            "167\n",
            "166\n",
            "165\n",
            "164\n",
            "163\n",
            "162\n",
            "161\n",
            "160\n",
            "159\n",
            "158\n",
            "157\n",
            "156\n",
            "155\n",
            "154\n",
            "153\n",
            "152\n",
            "151\n",
            "150\n",
            "149\n",
            "148\n",
            "147\n",
            "146\n",
            "145\n",
            "144\n",
            "143\n",
            "142\n",
            "141\n",
            "140\n",
            "139\n",
            "138\n",
            "137\n",
            "136\n",
            "135\n",
            "134\n",
            "133\n",
            "132\n",
            "131\n",
            "130\n",
            "129\n",
            "128\n",
            "127\n",
            "126\n",
            "125\n",
            "124\n",
            "123\n",
            "122\n",
            "121\n",
            "120\n",
            "119\n",
            "118\n",
            "117\n",
            "116\n",
            "115\n",
            "114\n",
            "113\n",
            "112\n",
            "111\n",
            "110\n",
            "109\n",
            "108\n",
            "107\n",
            "106\n",
            "105\n",
            "104\n",
            "103\n",
            "102\n",
            "101\n",
            "100\n",
            "99\n",
            "98\n",
            "97\n",
            "96\n",
            "95\n",
            "94\n",
            "93\n",
            "92\n",
            "91\n",
            "90\n",
            "89\n",
            "88\n",
            "87\n",
            "86\n",
            "85\n",
            "84\n",
            "83\n",
            "82\n",
            "81\n",
            "80\n",
            "79\n",
            "78\n",
            "77\n",
            "76\n",
            "75\n",
            "74\n",
            "73\n",
            "72\n",
            "71\n",
            "70\n",
            "69\n",
            "68\n",
            "67\n",
            "66\n",
            "65\n",
            "64\n",
            "63\n",
            "62\n",
            "61\n",
            "60\n",
            "59\n",
            "58\n",
            "57\n",
            "56\n",
            "55\n",
            "54\n",
            "53\n",
            "52\n",
            "51\n",
            "50\n",
            "49\n",
            "48\n",
            "47\n",
            "46\n",
            "45\n",
            "44\n",
            "43\n",
            "42\n",
            "41\n",
            "40\n",
            "39\n",
            "38\n",
            "37\n",
            "36\n",
            "35\n",
            "34\n",
            "33\n",
            "32\n",
            "31\n",
            "30\n",
            "29\n",
            "28\n",
            "27\n",
            "26\n",
            "25\n",
            "24\n",
            "23\n",
            "22\n",
            "21\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "0\n",
            "Generated 80 fake images.\n",
            "=== Evaluation Metrics ===\n",
            "FID: 429.7373\n",
            "IS_mean: 1.6577\n",
            "IS_std: 0.1798\n",
            "LPIPS: 0.2956\n",
            "{'FID': 429.7372731634043, 'IS_mean': 1.657655954360962, 'IS_std': 0.17983312904834747, 'LPIPS': 0.2955983579158783}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "siren_fourier",
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}